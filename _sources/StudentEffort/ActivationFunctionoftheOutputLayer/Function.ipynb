{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions and Their Derivatives\n",
    "\n",
    "![Mehrnoosh Ziaei](./assets/MehrnooshZiaei.jpg)\n",
    "- By Seyedeh Mehrnoosh Ziaei\n",
    "- Contact : s.mehrnooshziaei@gmail.com\n",
    "- Machine Learning - Autoencoders - Types of autoencoders - Octobor 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ReLU (Rectified Linear Unit)\n",
    "\n",
    "### Function:  \n",
    "$ f(x) = \\max(0, x) $  \n",
    "\n",
    "### Derivative:  \n",
    "$$\n",
    "    f'(x) = \n",
    "    \\begin{cases}  \n",
    "    0 & \\text{if } x < 0 \\\\  \n",
    "    1 & \\text{if } x > 0  \n",
    "    \\end{cases} \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "For $ x < 0 $, the function outputs 0 (constant), so the derivative is 0. For $ x > 0 $, the function is linear (slope of 1), so the derivative is 1.\n",
    "\n",
    "## 2. Leaky ReLU\n",
    "\n",
    "### Function:  \n",
    "\n",
    "$$\n",
    "    f(x) = \n",
    "    \\begin{cases}  \n",
    "    x & \\text{if } x > 0 \\\\  \n",
    "    \\alpha x & \\text{if } x \\leq 0  \n",
    "    \\end{cases} \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "\n",
    "$$\n",
    "    f'(x) = \n",
    "    \\begin{cases}  \n",
    "    1 & \\text{if } x > 0 \\\\  \n",
    "    \\alpha & \\text{if } x < 0  \n",
    "    \\end{cases} \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "Similar to ReLU, but for $ x < 0 $, the slope is $ \\alpha $ instead of 0.\n",
    "\n",
    "## 3. Sigmoid\n",
    "\n",
    "### Function:  \n",
    "\n",
    "$$\n",
    "    f(x) = \\frac{1}{1 + e^{-x}} \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "\n",
    "$$\n",
    "    f'(x) = f(x)(1 - f(x)) \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "Using the chain rule, we can derive that the derivative can be expressed in terms of the function itself.\n",
    "\n",
    "## 4. Tanh (Hyperbolic Tangent)\n",
    "\n",
    "### Function:  \n",
    "\n",
    "$$\n",
    "    f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "\n",
    "$$\n",
    "    f'(x) = 1 - \\tanh^2(x) \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "This is derived from the quotient rule or recognizing that the hyperbolic tangent function relates to exponential functions.\n",
    "\n",
    "## 5. Softmax\n",
    "\n",
    "### Function:  \n",
    "\n",
    "$$\n",
    "    f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}} \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "\n",
    "$$\n",
    "    f'(x_i) = \n",
    "    \\begin{cases}  \n",
    "    f(x_i)(1 - f(x_i)) & \\text{if } i = j \\\\  \n",
    "    -f(x_i)f(x_j) & \\text{if } i \\neq j  \n",
    "    \\end{cases} \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "The derivative is computed using the quotient rule and recognizing that softmax is used for multi-class probabilities.\n",
    "\n",
    "## 6. ELU (Exponential Linear Unit)\n",
    "\n",
    "### Function:  \n",
    "\n",
    "$$\n",
    "    f(x) = \n",
    "    \\begin{cases}  \n",
    "    x & \\text{if } x > 0 \\\\  \n",
    "    \\alpha(e^{x} - 1) & \\text{if } x \\leq 0  \n",
    "    \\end{cases} \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "\n",
    "$$\n",
    "    f'(x) = \n",
    "    \\begin{cases}  \n",
    "    1 & \\text{if } x > 0 \\\\  \n",
    "    \\alpha e^{x} & \\text{if } x < 0  \n",
    "    \\end{cases} \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "The derivative follows similarly to ReLU, but the negative part is based on the exponential function.\n",
    "\n",
    "## 7. Swish\n",
    "\n",
    "### Function:  \n",
    "$$\n",
    "    f(x) = x \\cdot \\text{sigmoid}(x) = \\frac{x}{1 + e^{-x}} \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "\n",
    "$$\n",
    "    f'(x) = \\text{sigmoid}(x) + x \\cdot \\text{sigmoid}(x)(1 - \\text{sigmoid}(x)) \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "This derivative can be found using the product rule and the chain rule.\n",
    "\n",
    "## 8. GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "### Function:  \n",
    "\n",
    "$$\n",
    "    f(x) = 0.5 x \\left( 1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715 x^3 \\right)\\right) \\right) \n",
    "\n",
    "$$\n",
    "\n",
    "### Derivative:  \n",
    "Deriving GELU is more complex, but it can be expressed as:  \n",
    "\n",
    "$$\n",
    "    f'(x) = 0.5 \\left( 1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715 x^3 \\right)\\right) + x \\cdot \\frac{d}{dx}(\\tanh) \\right) \n",
    "\n",
    "$$\n",
    "\n",
    "### Explanation:  \n",
    "The derivative is calculated using the product rule, chain rule, and recognizing the derivative of the hyperbolic tangent.\n",
    "\n",
    "## Summary of Derivatives\n",
    "\n",
    "| Activation Function | Derivative Formula |\n",
    "|---------------------|--------------------|\n",
    "| ReLU                | $ f'(x) = \\begin{cases}  0 & \\text{if } x < 0 \\\\  1 & \\text{if } x > 0 \\end{cases} $ |\n",
    "| Leaky ReLU          | $ f'(x) = \\begin{cases}  1 & \\text{if } x > 0 \\\\  \\alpha & \\text{if } x < 0 \\end{cases} $ |\n",
    "| Sigmoid             | $ f'(x) = f(x)(1 - f(x)) $ |\n",
    "| Tanh                | $ f'(x) = 1 - \\tanh^2(x) $ |\n",
    "| Softmax             | $ f'(x_i) = \\begin{cases}  f(x_i)(1 - f(x_i)) & \\text{if } i = j \\\\ -f(x_i)f(x_j) & \\text{if } i \\neq j \\end{cases} $ |\n",
    "| ELU                 | $ f'(x) = \\begin{cases}  1 & \\text{if } x > 0 \\\\  \\alpha e^{x} & \\text{if } x < 0 \\end{cases} $ |\n",
    "| Swish               | $ f'(x) = \\text{sigmoid}(x) + x \\cdot \\text{sigmoid}(x)(1 - \\text{sigmoid}(x)) $ |\n",
    "| GELU                | More complex; requires product and chain rule |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions Code and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # for numerical stability\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def swish(x):\n",
    "    return x / (1 + np.exp(-x))\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "# Generate input data\n",
    "x = np.linspace(-3, 3, 400)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title('ReLU')\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(x, leaky_relu(x))\n",
    "plt.title('Leaky ReLU')\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('Sigmoid')\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(x, tanh(x))\n",
    "plt.title('Tanh')\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(x, softmax(x))\n",
    "plt.title('Softmax')\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(x, elu(x))\n",
    "plt.title('ELU')\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.plot(x, swish(x))\n",
    "plt.title('Swish')\n",
    "\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.plot(x, gelu(x))\n",
    "plt.title('GELU')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Functions](./assets/Functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivates of Activation Functions Code and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # for numerical stability\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def swish(x):\n",
    "    return x / (1 + np.exp(-x))\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "# Define the derivative functions\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    s = softmax(x)\n",
    "    return np.diag(s) - np.outer(s, s)\n",
    "\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    return np.where(x > 0, 1, alpha * np.exp(x))\n",
    "\n",
    "def swish_derivative(x):\n",
    "    s = swish(x)\n",
    "    return s + x * sigmoid(x) * (1 - s)\n",
    "\n",
    "def gelu_derivative(x):\n",
    "    return 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * np.power(x, 3)))) + \\\n",
    "           0.5 * x * (1 - np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * np.power(x, 3)))**2) * \\\n",
    "           (np.sqrt(2/np.pi) * (1 + 3 * 0.044715 * np.power(x, 2)))\n",
    "\n",
    "# Generate input data\n",
    "x = np.linspace(-3, 3, 400)\n",
    "\n",
    "# Plotting the derivatives\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(x, relu_derivative(x))\n",
    "plt.title(\"ReLU Derivative\")\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(x, leaky_relu_derivative(x))\n",
    "plt.title(\"Leaky ReLU Derivative\")\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(x, sigmoid_derivative(x))\n",
    "plt.title(\"Sigmoid Derivative\")\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(x, tanh_derivative(x))\n",
    "plt.title(\"Tanh Derivative\")\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "# Softmax derivative is complex and requires a specific format\n",
    "# We'll skip this plot as it needs a 2D visualization\n",
    "plt.plot(x, np.zeros_like(x))  # Placeholder\n",
    "plt.title(\"Softmax Derivative (not plotted)\")\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(x, elu_derivative(x))\n",
    "plt.title(\"ELU Derivative\")\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.plot(x, swish_derivative(x))\n",
    "plt.title(\"Swish Derivative\")\n",
    "\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.plot(x, gelu_derivative(x))\n",
    "plt.title(\"GELU Derivative\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Derivations](./assets/Derivations.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
