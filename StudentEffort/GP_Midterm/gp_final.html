
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Midterm Exam - Gaussian Processes &#8212; Dr.Hadi Sadoghi Yazdi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'StudentEffort/GP_Midterm/gp_final';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Contact Me" href="../../Contact_Me.html" />
    <link rel="prev" title="Ridge Regression" href="../ridge/ridge.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Introduction_StudentEffort.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-light" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
    <img src="../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-dark pst-js-only" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Introduction_StudentEffort.html">
                    Enduring Efforts of Students
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Effort PR</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Whyprojection1.html">What is Projection?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Star%20Coordinates/Star_Coordinates.html">Star Coordinates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Covariance_Fekri/Covariance_Poorya.html">Covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linear-least-square-final/least-square.html">Linear Least Square Method</a></li>






<li class="toctree-l1"><a class="reference internal" href="../ECG/ecg.html">ECG: Measurement of heart rate and probability of heart attack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../correntropy/correntropy.html">Understanding Correntropy as a Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Normalization/Normalization_1.html">Data Normalization</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Chernoff/ChernoffEsri.html">Chernoff Faces in ESRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GloVe/GloVe1.html">GloVe: Word Representation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../LinkageClustering1/linkageclustering2.html">Linkage Clustering Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Word2Vec/Word2Vec.html">Word2Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../SOFM/SOFM.html">SOFM - Clustering and dimensionality reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../KNN/KNN.html">K Nearest-Neighbors KNN</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Bayes_Estimation_HW/Bayes_Estimation_HW.html">Posterior Distribution Derivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lms/lms.html">Least Mean Squares (LMS) Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Finall_PCA/finallPCA.html">Midterm Exam - PCA</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Effort ML</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../homeworkLLE/Homework_LLE.html">Locally Linear Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bias_variance_tradeoff/bias_variance_tardeoff.html">Bias-Variance Trade-Off</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Conjugate_Prior/Conjugate_Prior.html">Cojugate Prior</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ManifoldLearning/manifoldLearning.html">Manifold Learning Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ridge/ridge.html">Ridge Regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Midterm Exam - Gaussian Processes</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contact</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Contact_Me.html">Contact Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../About_Me.html">About Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git/issues/new?title=Issue%20on%20page%20%2FStudentEffort/GP_Midterm/gp_final.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/StudentEffort/GP_Midterm/gp_final.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Midterm Exam - Gaussian Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Midterm Exam - Gaussian Processes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#course-machine-learning">Course: Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semester-fall-1403">Semester: Fall 1403</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#authors"><strong>Authors</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">1. <strong>Model Setup</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution-on-weights">2. <strong>Prior Distribution on Weights</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-function">3. <strong>Likelihood Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-distribution-of-weights-training-phase">4. <strong>Posterior Distribution of Weights</strong> (<strong>Training Phase</strong>)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-derivation-of-the-posterior-distribution">Step-by-Step Derivation of the Posterior Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-0-what-is-the-product-of-two-gaussian-distributions"><strong>Step 0: What is the product of two Gaussian distributions?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-combine-the-exponentials"><strong>Step 1: Combine the Exponentials</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-expand-the-quadratic-terms"><strong>Step 2: Expand the Quadratic Terms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-term-y-w-top-x-top-y-w-top-x">First Term: <span class="math notranslate nohighlight">\(( (y - w^\top X)^\top (y - w^\top X) )\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#second-term-frac-1-2-w-top-sigma-p-1-w">Second Term: <span class="math notranslate nohighlight">\(( -\frac{1}{2} w^\top \Sigma_p^{-1} w )\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-combine-all-terms"><strong>Step 3: Combine All Terms</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-complete-the-square"><strong>Step 4: Complete the Square</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-derivative-gradient">First Derivative (Gradient)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#second-derivative-hessian">Second Derivative (Hessian)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-final-posterior-distribution"><strong>Step 5: Final Posterior Distribution</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-key-formulas"><strong>Summary of Key Formulas:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution-of-f">5. <strong>Predictive Distribution of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-mean-of-f"><strong>Predictive mean of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-variance-of-f"><strong>Predictive Variance of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-predictive-distribution">6. <strong>Final Predictive Distribution</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Summary of Key Formulas:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#handle-high-dimensional-and-kernels">Handle High-Dimensional and Kernels</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-kernels-in-gaussian-processes">When to Use Kernels in Gaussian Processes?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-define-the-covariance-between-data-points">1. <strong>To Define the Covariance Between Data Points</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-encode-assumptions-about-the-function-being-modeled">2. <strong>To Encode Assumptions About the Function Being Modeled</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-model-nonlinear-relationships">3. <strong>To Model Nonlinear Relationships</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-incorporate-prior-knowledge-about-the-data">4. <strong>To Incorporate Prior Knowledge about the Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-express-uncertainty-in-predictions">5. <strong>To Express Uncertainty in Predictions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-handle-high-dimensional-or-complex-input-spaces">6. <strong>To Handle High-Dimensional or Complex Input Spaces</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-model-complex-data-structures">7. <strong>To Model Complex Data Structures</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-achieve-non-parametric-flexibility">8. <strong>To Achieve Non-Parametric Flexibility</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-why-kernels-are-essential-in-gaussian-processes">Summary of Why Kernels Are Essential in Gaussian Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-kernel-functions-used-in-gaussian-processes">Common Kernel Functions Used in Gaussian Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-kernel-functions-in-gaussian-processes">Example of Kernel Functions in Gaussian Processes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-kernel-trick">Apply Kernel Trick</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-recall-the-prior-and-likelihood">Step 1: Recall the prior and likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-posterior-distribution">Step 2: Posterior Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-expressing-in-kernel-form">Step 3: Expressing in Kernel Form</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-mean-in-kernel-form">3.1 Posterior Mean in Kernel Form</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-variance-in-kernel-form">3.2 Posterior Variance in Kernel Form</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-result">Final Result</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-space-view-non-parametric-in-gaussian-processes">Function-Space View (Non parametric) in Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-over-functions">1. Prior Over Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-normal-distribution">2. Joint Normal Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-pdf">Joint PDF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-distribution">Marginal Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-distribution">Conditional Distribution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-over-functions">3. Posterior Over Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#incorporate-observational-noise">Incorporate Observational Noise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-example-of-gaussian-process-regression">Numerical Example of Gaussian Process Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-setup">Problem Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-compute-covariance-matrices">Step 1: Compute Covariance Matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-covariance-matrix-mathbf-k">1. Training covariance matrix <span class="math notranslate nohighlight">\(( \mathbf{K} )\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#test-covariance-vector-mathbf-k">2. Test covariance vector <span class="math notranslate nohighlight">\(( \mathbf{k}_* )\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-for-x-4">Example for ( x_* = -4 ):</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-for-all-test-points">Computation for all test points:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-adjusted-covariance-matrix-mathbf-k-y">3. Noise-adjusted covariance matrix <span class="math notranslate nohighlight">\(( \mathbf{K}_y )\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-predictive-mean-and-variance">Step 2: Predictive Mean and Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-mean-mu-x">1. Predictive mean <span class="math notranslate nohighlight">\(( \mu(x_*) )\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-variance-sigma-2-x">2. Predictive variance <span class="math notranslate nohighlight">\(( \sigma^2(x_*) )\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-predictions-for-all-test-points">Step 3: Predictions for All Test Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-confidence-intervals">Step 4: Confidence Intervals</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Example:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-conditional-distribution-formula">Proof of Conditional Distribution Formula</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-scenarios-and-insights-in-gaussian-processes">Key Scenarios and Insights in Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#secenario-1-no-data-gp-prior">Secenario 1: No Data (GP Prior)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-2-similarity-to-one-training-point">Scenario 2: Similarity to One Training Point</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-3-outlier-data">Scenario 3: Outlier Data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-4-dense-noisy-data">Scenario 4: Dense Noisy Data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-overview">General Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-overview">Scenario Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insights">Key Insights</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-example">Numerical Example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-1-no-data-prior">Scenario 1: No Data (Prior)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#training-targets">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#training-covariance-matrix-k">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-covariance-matrix-k">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#test-covariance-matrix-k">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-at-test-point">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-at-test-point">Variance at Test Point:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Scenario 2: Similarity to One Training Point</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Variance at Test Point:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-3-outlier-in-training-data">Scenario 3: Outlier in Training Data</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Variance at Test Point:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Scenario 4: Dense Noisy Data</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Variance at Test Point:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="midterm-exam-gaussian-processes">
<h1>Midterm Exam - Gaussian Processes<a class="headerlink" href="#midterm-exam-gaussian-processes" title="Link to this heading">#</a></h1>
<section id="course-machine-learning">
<h2>Course: Machine Learning<a class="headerlink" href="#course-machine-learning" title="Link to this heading">#</a></h2>
</section>
<section id="semester-fall-1403">
<h2>Semester: Fall 1403<a class="headerlink" href="#semester-fall-1403" title="Link to this heading">#</a></h2>
<p><img alt="students" src="../../_images/students1.jpg" /></p>
</section>
<section id="authors">
<h2><strong>Authors</strong><a class="headerlink" href="#authors" title="Link to this heading">#</a></h2>
<p>This document was prepared by:</p>
<ul class="simple">
<li><p>Mahdieh Alizadeh</p></li>
<li><p>Mehrnoosh Ziaei</p></li>
<li><p>Behzad Sabeti</p></li>
<li><p>Payam Parvazmanesh</p></li>
<li><p>Poorya Fekri</p></li>
<li><p>Erfan fakoor</p></li>
</ul>
<p>Special thanks to the Machine Learning professor <strong>Dr. Hadi Sadoghi Yazdi</strong> for his guidance and contributions.</p>
</section>
<section id="model-setup">
<h2>1. <strong>Model Setup</strong><a class="headerlink" href="#model-setup" title="Link to this heading">#</a></h2>
<p>We model the relationship between the observed outputs <span class="math notranslate nohighlight">\(( y_i )\)</span> and input vectors <span class="math notranslate nohighlight">\(( x_i )\)</span> with a linear function:</p>
<div class="math notranslate nohighlight">
\[
y_i = w^\top x_i + \epsilon_i,
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(( y_i )\)</span> is the observed output for the <span class="math notranslate nohighlight">\(( i )-th\)</span> data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(( x_i \in \mathbb{R}^d )\)</span> is the input vector for the <span class="math notranslate nohighlight">\(( i )-th\)</span> data point, where <span class="math notranslate nohighlight">\(( d )\)</span> is the number of features.</p></li>
<li><p><span class="math notranslate nohighlight">\(( w \in \mathbb{R}^d )\)</span> is the weight vector (unknown parameters).</p></li>
<li><p><span class="math notranslate nohighlight">\(( \epsilon_i \sim \mathcal{N}(0, \sigma^2) )\)</span> is Gaussian noise with mean 0 and variance <span class="math notranslate nohighlight">\(( \sigma^2 )\)</span>.</p></li>
</ul>
<p>The dataset consists of <span class="math notranslate nohighlight">\(( n )\)</span> independent observations <span class="math notranslate nohighlight">\(( \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} )\)</span>.</p>
</section>
<section id="prior-distribution-on-weights">
<h2>2. <strong>Prior Distribution on Weights</strong><a class="headerlink" href="#prior-distribution-on-weights" title="Link to this heading">#</a></h2>
<p>We assume a <strong>Gaussian prior</strong> on the weight vector <span class="math notranslate nohighlight">\(( w )\)</span>, which expresses our belief about the weights before seeing any data. The prior is given by:</p>
<div class="math notranslate nohighlight">
\[
p(w) = \mathcal{N}(0, \Sigma_p),
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(( \Sigma_w \in \mathbb{R}^{d \times d} )\)</span> is the covariance matrix of the prior distribution on <span class="math notranslate nohighlight">\(( w )\)</span>. This can be chosen based on prior knowledge about the weights or set as <span class="math notranslate nohighlight">\(( \Sigma_p = \lambda I )\)</span>, where <span class="math notranslate nohighlight">\(( I )\)</span> is the identity matrix and <span class="math notranslate nohighlight">\(( \lambda &gt; 0 )\)</span> is a regularization parameter.</p></li>
</ul>
</section>
<section id="likelihood-function">
<h2>3. <strong>Likelihood Function</strong><a class="headerlink" href="#likelihood-function" title="Link to this heading">#</a></h2>
<p>The likelihood of the observed data <span class="math notranslate nohighlight">\(( y = [y_1, y_2, \dots, y_n]^\top )\)</span> given the inputs <span class="math notranslate nohighlight">\(( X = [x_1^\top, x_2^\top, \dots, x_n^\top]^\top )\)</span> and weights <span class="math notranslate nohighlight">\(( w )\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
   p(y|X,w) = \prod_{i=1}^n p(y_i|x_i, w) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2_n}} \exp\left( -\frac{(y_i - w^\top x_i)^2}{2\sigma^2_n} \right)
   \]</div>
<div class="math notranslate nohighlight">
\[
   = \frac{1}{(2\pi\sigma^2_n)^{n/2}} \exp\left( -\frac{1}{2\sigma^2_n} \|y - w^\top X\|^2 \right)
   \]</div>
<div class="math notranslate nohighlight">
\[
   p(y|X,w)=\mathcal{N}(w^\top X, \sigma^2)
   \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> is the vector of observed target values.</p></li>
<li><p><span class="math notranslate nohighlight">\( X \)</span> is the matrix of input vectors.</p></li>
<li><p><span class="math notranslate nohighlight">\( \sigma^2_n \)</span> is the variance of the noise.</p></li>
<li><p>The likelihood <span class="math notranslate nohighlight">\( p(y|X,w) \)</span> represents the probability of observing <span class="math notranslate nohighlight">\( y \)</span> given <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( w \)</span>.</p></li>
</ul>
</section>
<section id="posterior-distribution-of-weights-training-phase">
<h2>4. <strong>Posterior Distribution of Weights</strong> (<strong>Training Phase</strong>)<a class="headerlink" href="#posterior-distribution-of-weights-training-phase" title="Link to this heading">#</a></h2>
<p>Using <strong>Bayes’ rule</strong>, the posterior distribution of <span class="math notranslate nohighlight">\(( w )\)</span> given the data <span class="math notranslate nohighlight">\(( X )\)</span> and <span class="math notranslate nohighlight">\(( y )\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
   p(w|y,X) = \frac{p(y|X,w) p(w)}{p(y|X)}
   \]</div>
<p>As we want to calculate <span class="math notranslate nohighlight">\(p(w|y,X)\)</span> We can assume <span class="math notranslate nohighlight">\(p(y|X)\)</span> is a constant and this term is absorbed in the proportion.
$<span class="math notranslate nohighlight">\(
   p(w|y,X) \propto p(y|X,w)*p(w)
   \)</span>$</p>
<p>As we know from prior that <span class="math notranslate nohighlight">\(w \sim \mathcal{N}(0, \Sigma_p)\)</span>, so <span class="math notranslate nohighlight">\(p(w|y,X) \propto \exp\left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right) \)</span></p>
<p>and also we have <span class="math notranslate nohighlight">\(y = x^\top w + \epsilon\)</span>. So when we know <span class="math notranslate nohighlight">\(X,w\)</span> then <span class="math notranslate nohighlight">\(y\)</span>  behaves like  <span class="math notranslate nohighlight">\(\epsilon\)</span> shifting in accordance with <span class="math notranslate nohighlight">\(x^\top w\)</span>. It means that <span class="math notranslate nohighlight">\(y \sim \mathcal{N}(X^\top w, \sigma^2_n)\)</span> Therefore, <span class="math notranslate nohighlight">\(p(y|X,w) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - X^\top w)^\top (y - X^\top w) \right)\)</span></p>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[
   p(w|X,y) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - w^\top X)^\top (y - w^\top X) \right) \exp\left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right)
   \]</div>
<section id="step-by-step-derivation-of-the-posterior-distribution">
<h3>Step-by-Step Derivation of the Posterior Distribution<a class="headerlink" href="#step-by-step-derivation-of-the-posterior-distribution" title="Link to this heading">#</a></h3>
<p>We start with the following expression for the posterior distribution of ( w ) given the data ( X ) and ( y ):</p>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - w^\top X)^\top (y - w^\top X) \right) \exp\left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right)
\]</div>
</section>
<section id="step-0-what-is-the-product-of-two-gaussian-distributions">
<h3><strong>Step 0: What is the product of two Gaussian distributions?</strong><a class="headerlink" href="#step-0-what-is-the-product-of-two-gaussian-distributions" title="Link to this heading">#</a></h3>
<p>In the link below we can see the answer:</p>
<p><a class="reference external" href="https://www.desmos.com/calculator/qtua9ikyhc">Ploting the product of two Gaussian distributions</a></p>
<p>It’s interesting to look at how the new mean and the covariance is computed. New mean is actually weighted average of the old means weighted by the opposite covariances. Let’s assume that the distribution of <span class="math notranslate nohighlight">\(X_1\)</span> is skinny and has low variance compared to the distribution of <span class="math notranslate nohighlight">\(X_2\)</span>. The second Gaussian here has a very large covariance so it’s a very broad and very wide uncertain distribution. Whereas <span class="math notranslate nohighlight">\(X_1\)</span> is really narrow and has a very small covariance, so the random variable <span class="math notranslate nohighlight">\(X_1\)</span> has much less uncertainty than random variable <span class="math notranslate nohighlight">\(X_2\)</span>. Then the result of the intersection will be strongly influenced by <span class="math notranslate nohighlight">\(\mu_1\)</span>, so by the first random variable <span class="math notranslate nohighlight">\(X_1\)</span> because <span class="math notranslate nohighlight">\(Σ_2\)</span> is large and <span class="math notranslate nohighlight">\(Σ_1\)</span> is relatively small. In same manner the influence of <span class="math notranslate nohighlight">\(µ_2\)</span> will be relatively limited.</p>
<p>New covariance is essentially the inverse of the sum of the individual covariance inverses. The new variance <span class="math notranslate nohighlight">\(\sigma_2\)</span> is twice the harmonic mean of the individual variances.</p>
<p>In short it means that the product of two gaussians PDFs g1(x) and g2(x) is a scaled gaussian PDF. Also the uncertainty shrinks, so the covariance shrinks, so if you combine two Gaussians then the result will be narrower and more certain than the original covariances. And the mean of the new distribution would be proportionately closer to the skinnier distribution.</p>
</section>
<section id="step-1-combine-the-exponentials">
<h3><strong>Step 1: Combine the Exponentials</strong><a class="headerlink" href="#step-1-combine-the-exponentials" title="Link to this heading">#</a></h3>
<p>We combine the two exponential terms from the likelihood and prior:</p>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - w^\top X)^\top (y - w^\top X) - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
\]</div>
<p>Now we expand the quadratic terms inside the exponent.</p>
</section>
<section id="step-2-expand-the-quadratic-terms">
<h3><strong>Step 2: Expand the Quadratic Terms</strong><a class="headerlink" href="#step-2-expand-the-quadratic-terms" title="Link to this heading">#</a></h3>
<section id="first-term-y-w-top-x-top-y-w-top-x">
<h4>First Term: <span class="math notranslate nohighlight">\(( (y - w^\top X)^\top (y - w^\top X) )\)</span><a class="headerlink" href="#first-term-y-w-top-x-top-y-w-top-x" title="Link to this heading">#</a></h4>
<p>We expand the first quadratic term as follows:</p>
<div class="math notranslate nohighlight">
\[
(y - w^\top X)^\top (y - w^\top X) = y^\top y - 2 y^\top w^\top X + w^\top X^\top X w
\]</div>
<p>Thus, the first term in the exponent becomes:</p>
<div class="math notranslate nohighlight">
\[
-\frac{1}{2\sigma^2_n} \left( y^\top y - 2 y^\top w^\top X + w^\top X^\top X w \right)
\]</div>
</section>
<section id="second-term-frac-1-2-w-top-sigma-p-1-w">
<h4>Second Term: <span class="math notranslate nohighlight">\(( -\frac{1}{2} w^\top \Sigma_p^{-1} w )\)</span><a class="headerlink" href="#second-term-frac-1-2-w-top-sigma-p-1-w" title="Link to this heading">#</a></h4>
<p>This term remains unchanged:</p>
<div class="math notranslate nohighlight">
\[
-\frac{1}{2} w^\top \Sigma_p^{-1} w
\]</div>
</section>
</section>
<section id="step-3-combine-all-terms">
<h3><strong>Step 3: Combine All Terms</strong><a class="headerlink" href="#step-3-combine-all-terms" title="Link to this heading">#</a></h3>
<p>Now, we combine the expanded terms:</p>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) \propto \exp\left( -\frac{1}{2\sigma^2_n} y^\top y + \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
\]</div>
<p>Since <span class="math notranslate nohighlight">\(( y^\top y )\)</span> is independent of <span class="math notranslate nohighlight">\(( w )\)</span>, we can ignore this constant term, and the expression simplifies to:</p>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) \propto \exp\left( \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
\]</div>
</section>
<section id="step-4-complete-the-square">
<h3><strong>Step 4: Complete the Square</strong><a class="headerlink" href="#step-4-complete-the-square" title="Link to this heading">#</a></h3>
<p>Given the expression:</p>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) \propto \exp\left( \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
\]</div>
<p>This expression is quadratic in <span class="math notranslate nohighlight">\((w)\)</span>, representing a Gaussian distribution. Therefore:</p>
<ol class="arabic simple">
<li><p>The <strong>first derivative</strong> with respect to <span class="math notranslate nohighlight">\(( w )\)</span> gives the critical point, which corresponds to the mean (MAP estimate) of the Gaussian.</p></li>
<li><p>The <strong>second derivative</strong> (Hessian) determines the curvature, and its negative inverse provides the covariance matrix of the posterior distribution.</p></li>
</ol>
<section id="first-derivative-gradient">
<h4>First Derivative (Gradient)<a class="headerlink" href="#first-derivative-gradient" title="Link to this heading">#</a></h4>
<p>Let <span class="math notranslate nohighlight">\((f(w))\)</span> denote the exponent of the posterior:</p>
<div class="math notranslate nohighlight">
\[
f(w) = \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w
\]</div>
<p>The gradient <span class="math notranslate nohighlight">\(( \nabla_w f(w))\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w f(w) = \frac{\partial}{\partial w} \left( \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
\]</div>
<p>Differentiating each term:</p>
<ol class="arabic simple">
<li><p>For the first term:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial w} \left( \frac{1}{\sigma^2_n} y^\top w^\top X \right) = \frac{1}{\sigma^2_n} X^\top y
\]</div>
<ol class="arabic simple" start="2">
<li><p>For the second term:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial w} \left( -\frac{1}{2\sigma^2_n} w^\top X^\top X w \right) = -\frac{1}{\sigma^2_n} X^\top X w
\]</div>
<ol class="arabic simple" start="3">
<li><p>For the third term:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial w} \left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right) = -\Sigma_p^{-1} w
\]</div>
<p>Combining these results:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w f(w) = \frac{1}{\sigma^2_n} X^\top y - \frac{1}{\sigma^2_n} X^\top X w - \Sigma_p^{-1} w
\]</div>
<p>Setting the gradient to zero to find the critical point <span class="math notranslate nohighlight">\(( \bar{w})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{\sigma^2_n} X^\top y - \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right) \bar{w} = 0
\]</div>
<p>Solving for <span class="math notranslate nohighlight">\((\bar{w})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\bar{w} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y
\]</div>
</section>
<section id="second-derivative-hessian">
<h4>Second Derivative (Hessian)<a class="headerlink" href="#second-derivative-hessian" title="Link to this heading">#</a></h4>
<p>The Hessian matrix <span class="math notranslate nohighlight">\((H)\)</span> is the second derivative of <span class="math notranslate nohighlight">\((f(w))\)</span> with respect to <span class="math notranslate nohighlight">\((w)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w^2 f(w) = \frac{\partial}{\partial w} \left( -\frac{1}{\sigma^2_n} X^\top X w - \Sigma_p^{-1} w \right)
\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[
H = -\left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)
\]</div>
<p>then as we said before:
<span class="math notranslate nohighlight">\(A=-(H^{-1})\)</span></p>
</section>
</section>
<section id="step-5-final-posterior-distribution">
<h3><strong>Step 5: Final Posterior Distribution</strong><a class="headerlink" href="#step-5-final-posterior-distribution" title="Link to this heading">#</a></h3>
<p>Thus, the posterior distribution of <span class="math notranslate nohighlight">\(( w )\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) = \mathcal{N}\left( \bar{w}, \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \right)
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>The <strong>posterior mean</strong> <span class="math notranslate nohighlight">\(( \bar{w} )\)</span> is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\bar{w} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y
\]</div>
<ul class="simple">
<li><p>The <strong>posterior covariance</strong> <span class="math notranslate nohighlight">\(( A^{-1} )\)</span> is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A^{-1} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1}
\]</div>
</section>
<section id="summary-of-key-formulas">
<h3><strong>Summary of Key Formulas:</strong><a class="headerlink" href="#summary-of-key-formulas" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The <strong>posterior distribution</strong> is a Gaussian:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(w \mid X, y) = \mathcal{N}\left( \bar{w}, A^{-1} \right)
\]</div>
<ul class="simple">
<li><p>The <strong>posterior mean</strong> <span class="math notranslate nohighlight">\(( \bar{w} )\)</span> is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\bar{w} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y
\]</div>
<ul class="simple">
<li><p>The <strong>posterior covariance</strong> <span class="math notranslate nohighlight">\(( A^{-1} )\)</span> is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
A^{-1} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1}
\]</div>
<ul class="simple">
<li><p>What do these mean and variance mean?</p></li>
</ul>
<ol class="arabic simple">
<li><p>What we see in the mean formula means that the mean of the posterior distribution tends to the mean of our prior knowledge if the uncertainty of the training data is high or the number of data is small.</p></li>
<li><p>On the other hand, if the uncertainty of the training data is low or the number of data is large, the mean of the posterior distribution tends to the mean of the training data.</p></li>
<li><p>If we look closely, <span class="math notranslate nohighlight">\(y\)</span> has appeared in the mean but not in the covariance. This means that the covariance and uncertainty we estimate are independent of the observed data, and this is one of the disadvantages of Gaussian processes.</p></li>
<li><p>The covariance of the posterior distribution depends only on prior knowledge and training data.</p></li>
<li><p>As we see in the covariance formula, if the uncertainty of the training data is large or the number of training data is small, the posterior distribution determines its uncertainty based on the uncertainty of the prior knowledge.</p></li>
<li><p>On the other hand, if the volume of training data is large enough or the uncertainty of the training data is low; the posterior distribution of uncertainty determines its uncertainty based on the uncertainty of the training data.</p></li>
</ol>
<p>This concludes the derivation of the posterior distribution <span class="math notranslate nohighlight">\(( p(w \mid X, y) )\)</span> with <span class="math notranslate nohighlight">\(( w^\top X )\)</span> notation.</p>
</section>
</section>
<section id="predictive-distribution-of-f">
<h2>5. <strong>Predictive Distribution of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong><a class="headerlink" href="#predictive-distribution-of-f" title="Link to this heading">#</a></h2>
<p>To make predictions for a test case, we average over all possible parameter predictive distribution values, weighted by their posterior probability. This is in contrast to non-Bayesian schemes, where a single parameter is typically chosen by some criterion. Thus, the predictive distribution for <span class="math notranslate nohighlight">\(( f_* )\)</span>, <span class="math notranslate nohighlight">\(( f(x_*) )\)</span> at <span class="math notranslate nohighlight">\(( x_* )\)</span> is given by averaging the output of all possible linear models with respect to the Gaussian posterior:</p>
<div class="math notranslate nohighlight">
\[
p(f_* \mid x_*, X, y) = \int p(f_* \mid x_*, w) p(w \mid X, y) \, dw.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(( p(y_* \mid x_*, w) = \mathcal{N}( w^\top x_*, \sigma^2) )\)</span>, the predictive distribution is also Gaussian. The mean and variance of the predictive distribution can be computed as follows.</p>
<section id="predictive-mean-of-f">
<h3><strong>Predictive mean of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong>:<a class="headerlink" href="#predictive-mean-of-f" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[f_* \mid X,y,x_*] 
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[w^\top x_* \mid X,y,x_*] 
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[x_*^\top w \mid X,y,x_*] 
\]</div>
<div class="math notranslate nohighlight">
\[
x_*^\top \mathbb{E}[ w \mid X,y]
\]</div>
<div class="math notranslate nohighlight">
\[
x_*^\top \bar{w}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{mean}(f_*) = x_*^\top(\left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y)
\]</div>
</section>
<section id="predictive-variance-of-f">
<h3><strong>Predictive Variance of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong><a class="headerlink" href="#predictive-variance-of-f" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\text{Var}(f_*) = \mathbb{E}[ (f_* - \mathbb{E}[f_* \mid X,y,x_*])^2 \mid X,y,x_* ]
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[ (w^\top x_* - \bar{w}^\top x_*)^2 \mid X,y,x_*]
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[((w- \bar{w})^\top x_*)^2 \mid X,y,x_*]
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[x_*^\top (w-\bar{w})(w-\bar{w})^\top x_* \mid X,y,x_*]
\]</div>
<div class="math notranslate nohighlight">
\[
x_*^\top \mathbb{E}[ (w-\bar{w})(w-\bar{w})^\top \mid X,y] x_*
\]</div>
<div class="math notranslate nohighlight">
\[
\text{Var}(f_*) = x_*^\top A^{-1} x_*
\]</div>
</section>
</section>
<section id="final-predictive-distribution">
<h2>6. <strong>Final Predictive Distribution</strong><a class="headerlink" href="#final-predictive-distribution" title="Link to this heading">#</a></h2>
<p>The predictive distribution for <span class="math notranslate nohighlight">\(( f_* )\)</span> is therefore:</p>
<div class="math notranslate nohighlight">
\[
f_* \mid x_*, X, y \sim \mathcal{N}(\mu_*, \sigma_*^2),
\]</div>
<p>where:</p>
<ul class="simple">
<li><p>The <strong>predictive mean</strong> is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mu_* = x_*^\top A^{-1} \left( \frac{1}{\sigma^2} X^\top y \right),
\]</div>
<ul class="simple">
<li><p>The <strong>predictive variance</strong> is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_*^2 = x_*^\top A^{-1} x_* 
\]</div>
</section>
<section id="id1">
<h2>Summary of Key Formulas:<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Prior</strong>: <span class="math notranslate nohighlight">\( p(w) = \mathcal{N}(0, \Sigma_p) \)</span></p></li>
<li><p><strong>Likelihood</strong>: <span class="math notranslate nohighlight">\( p(y \mid X, w) = \mathcal{N}( w^\top X, \sigma^2) \)</span></p></li>
<li><p><strong>Posterior</strong>: <span class="math notranslate nohighlight">\( p(w \mid X, y) = \mathcal{N}(\bar{w}, A^{-1}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{w} = A^{-1} \left( \frac{1}{\sigma^2} X^\top y \right) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( A^{-1} = \left( \Sigma_p^{-1} + \frac{1}{\sigma^2} X^\top X \right)^{-1} \)</span></p></li>
<li><p><strong>Predictive Mean</strong>: <span class="math notranslate nohighlight">\(\mu_* = x_*^\top A^{-1} \left( \frac{1}{\sigma^2} X^\top y \right)\)</span></p></li>
<li><p><strong>Predictive Variance</strong>: <span class="math notranslate nohighlight">\(\sigma_*^2 = x_*^\top A^{-1} x_*\)</span></p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="handle-high-dimensional-and-kernels">
<h1>Handle High-Dimensional and Kernels<a class="headerlink" href="#handle-high-dimensional-and-kernels" title="Link to this heading">#</a></h1>
<p>Given the formula for the posterior distribution of the function values <span class="math notranslate nohighlight">\( f_* \)</span> at a test point <span class="math notranslate nohighlight">\( x_* \)</span>:</p>
<div class="math notranslate nohighlight">
\[
f_* | x_*, X, y \sim \mathcal{N}\left( \frac{1}{\sigma^2_n} \phi(x_*)^\top A^{-1} \phi^\top y, \phi(x_*)^\top A^{-1} \phi(x_*) \right)
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \phi(x_*) \)</span> is the feature map of the test point <span class="math notranslate nohighlight">\( x_* \)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\( \Phi = [\phi(x_1), \phi(x_2), ..., \phi(x_N)] \)</span> is the matrix of feature maps for the training data,</p></li>
<li><p><span class="math notranslate nohighlight">\( A = \sigma_n^{-2} \Phi \Phi^\top + \Sigma_p^{-1} \)</span> is the precision matrix, combining the noise precision and the prior precision.</p></li>
</ul>
<p>We wish to express this in terms of <strong>kernels</strong> because of several reasons:
Kernels are used in various contexts, particularly when working with models that need to operate in high-dimensional or complex input spaces. Here’s a breakdown of when and why you should use kernels:</p>
<section id="when-to-use-kernels-in-gaussian-processes">
<h2>When to Use Kernels in Gaussian Processes?<a class="headerlink" href="#when-to-use-kernels-in-gaussian-processes" title="Link to this heading">#</a></h2>
<p>Kernels (also known as covariance functions) play a crucial role in Gaussian Processes (GP), which are non-parametric models used for regression and classification tasks. In GP, kernels define the covariance structure of the underlying function you’re trying to model. Here’s why and when kernels are used in Gaussian Processes:</p>
<section id="to-define-the-covariance-between-data-points">
<h3>1. <strong>To Define the Covariance Between Data Points</strong><a class="headerlink" href="#to-define-the-covariance-between-data-points" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The kernel defines the covariance between any two points in the input space. This is central to Gaussian Processes because GPs assume that any set of function values has a joint Gaussian distribution, and the covariance matrix is defined by the kernel.</p></li>
<li><p><strong>Example</strong>: If you have input points <span class="math notranslate nohighlight">\( X \)</span> and want to predict the function value at a new point <span class="math notranslate nohighlight">\( x_* \)</span>, the kernel specifies how similar or correlated <span class="math notranslate nohighlight">\( x_* \)</span> is to the training points <span class="math notranslate nohighlight">\( X \)</span>.</p></li>
</ul>
</section>
<section id="to-encode-assumptions-about-the-function-being-modeled">
<h3>2. <strong>To Encode Assumptions About the Function Being Modeled</strong><a class="headerlink" href="#to-encode-assumptions-about-the-function-being-modeled" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Kernels allow you to express assumptions about the properties of the underlying function, such as smoothness, periodicity, or noise levels.</p>
<ul>
<li><p><strong>Smoothness</strong>: The function may be assumed to be continuous and differentiable (e.g., using the RBF kernel).</p></li>
<li><p><strong>Periodicity</strong>: You may assume that the function repeats itself over time (e.g., using periodic kernels).</p></li>
<li><p><strong>Noise</strong>: Kernels can also model the noise in the data (e.g., adding a noise kernel to account for measurement errors).</p></li>
</ul>
</li>
<li><p><strong>Example</strong>: In GP regression, the choice of kernel controls the smoothness of the predicted function and how strongly the model relies on nearby data points.</p></li>
</ul>
</section>
<section id="to-model-nonlinear-relationships">
<h3>3. <strong>To Model Nonlinear Relationships</strong><a class="headerlink" href="#to-model-nonlinear-relationships" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Kernels are particularly useful when the relationship between the input variables and the output is nonlinear. By using a kernel function, the GP model can implicitly map the data into a higher-dimensional space, enabling it to model complex, nonlinear relationships without explicitly transforming the data.</p></li>
<li><p><strong>Example</strong>: The Radial Basis Function (RBF) kernel (also known as the Gaussian kernel) is widely used to capture smooth, nonlinear relationships in the data. It allows the GP to make accurate predictions for nonlinear functions, such as those found in time series or spatial data.</p></li>
</ul>
</section>
<section id="to-incorporate-prior-knowledge-about-the-data">
<h3>4. <strong>To Incorporate Prior Knowledge about the Data</strong><a class="headerlink" href="#to-incorporate-prior-knowledge-about-the-data" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In GP, the kernel can incorporate prior knowledge or domain expertise about the function you’re modeling. For example, if you know the function has periodic behavior, you can use a periodic kernel.</p></li>
<li><p><strong>Example</strong>: If you’re modeling temperature over time, a periodic kernel would encode the assumption that temperatures repeat with a certain period.</p></li>
</ul>
</section>
<section id="to-express-uncertainty-in-predictions">
<h3>5. <strong>To Express Uncertainty in Predictions</strong><a class="headerlink" href="#to-express-uncertainty-in-predictions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Gaussian Processes are probabilistic models, and the kernel not only helps make predictions but also quantifies the uncertainty in those predictions. This is especially useful in settings where you need to understand the confidence in the model’s output, such as in active learning or optimization tasks.</p></li>
<li><p><strong>Example</strong>: The variance predicted by the GP, derived from the kernel, quantifies the uncertainty in the model’s prediction for a given input.</p></li>
</ul>
</section>
<section id="to-handle-high-dimensional-or-complex-input-spaces">
<h3>6. <strong>To Handle High-Dimensional or Complex Input Spaces</strong><a class="headerlink" href="#to-handle-high-dimensional-or-complex-input-spaces" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Many problems, especially in machine learning and statistics, involve high-dimensional or complex data. Kernels provide a way to compute dot products in high-dimensional feature spaces efficiently (using the “kernel trick”), allowing Gaussian Processes to model data in these spaces without explicitly computing the higher-dimensional representation.</p></li>
<li><p><strong>Example</strong>: In spatial data, where each data point is associated with a location, kernels such as the squared exponential (RBF) kernel or Matérn kernel help model the spatial correlation of the data without the need to define an explicit transformation to higher dimensions.</p></li>
</ul>
</section>
<section id="to-model-complex-data-structures">
<h3>7. <strong>To Model Complex Data Structures</strong><a class="headerlink" href="#to-model-complex-data-structures" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Kernels can be used to handle complex data types such as time series, images, or text. By using specialized kernels, GPs can model relationships that are difficult to capture with standard linear models.</p></li>
<li><p><strong>Example</strong>: In time series forecasting, a kernel that accounts for temporal correlations (such as a temporal RBF kernel) can be used to model how future values depend on past values in a continuous-time process.</p></li>
</ul>
</section>
<section id="to-achieve-non-parametric-flexibility">
<h3>8. <strong>To Achieve Non-Parametric Flexibility</strong><a class="headerlink" href="#to-achieve-non-parametric-flexibility" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Gaussian Processes are non-parametric, meaning they do not assume a specific form for the function being modeled. Instead, the kernel function defines the space of possible functions. This non-parametric nature is particularly useful when you have little prior knowledge about the form of the function.</p></li>
<li><p><strong>Example</strong>: A GP with a properly chosen kernel can fit a wide range of functions, from smooth and continuous functions to those with more abrupt changes, without specifying a parametric model in advance.</p></li>
</ul>
</section>
<section id="summary-of-why-kernels-are-essential-in-gaussian-processes">
<h3>Summary of Why Kernels Are Essential in Gaussian Processes<a class="headerlink" href="#summary-of-why-kernels-are-essential-in-gaussian-processes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Covariance modeling</strong>: Kernels define how data points are related through their covariance, essential for GP.</p></li>
<li><p><strong>Encoding assumptions</strong>: Kernels allow you to express assumptions about the smoothness, periodicity, and other properties of the function.</p></li>
<li><p><strong>Nonlinearity</strong>: Kernels enable GPs to model complex, nonlinear relationships between inputs and outputs.</p></li>
<li><p><strong>Prior knowledge</strong>: Kernels provide a way to incorporate domain knowledge about the data.</p></li>
<li><p><strong>Uncertainty quantification</strong>: In GPs, kernels help quantify the uncertainty in predictions.</p></li>
<li><p><strong>High-dimensional data</strong>: Kernels allow GPs to efficiently handle high-dimensional input spaces.</p></li>
<li><p><strong>Complex data</strong>: Specialized kernels enable GPs to handle complex data structures, such as time series and spatial data.</p></li>
<li><p><strong>Non-parametric flexibility</strong>: GPs with kernels provide a flexible, non-parametric approach to modeling data.</p></li>
</ul>
</section>
<section id="common-kernel-functions-used-in-gaussian-processes">
<h3>Common Kernel Functions Used in Gaussian Processes<a class="headerlink" href="#common-kernel-functions-used-in-gaussian-processes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Linear Kernel</strong>: Suitable for modeling linear relationships.</p></li>
<li><p><strong>Radial Basis Function (RBF) / Gaussian Kernel</strong>: Captures smooth, continuous functions, widely used in GP regression.</p></li>
<li><p><strong>Matérn Kernel</strong>: More flexible than the RBF kernel, allowing for different degrees of smoothness.</p></li>
<li><p><strong>Periodic Kernel</strong>: Useful for modeling periodic functions, such as in time series with a repeating pattern.</p></li>
<li><p><strong>Noise Kernel</strong>: Adds a noise term to model observation noise in the data.</p></li>
</ul>
</section>
<section id="example-of-kernel-functions-in-gaussian-processes">
<h3>Example of Kernel Functions in Gaussian Processes<a class="headerlink" href="#example-of-kernel-functions-in-gaussian-processes" title="Link to this heading">#</a></h3>
<p>The most commonly used kernel in Gaussian Processes is the <strong>Radial Basis Function (RBF)</strong> kernel, also known as the <strong>Gaussian Kernel</strong>, which is defined as:</p>
<div class="math notranslate nohighlight">
\[
k(x, x') = \exp\left(-\frac{|x - x'|^2}{2 \sigma^2}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\( \sigma \)</span> is a hyperparameter that controls the smoothness of the function being modeled.</p>
<p>Another widely used kernel is the <strong>Matérn Kernel</strong>, which is defined as:</p>
<div class="math notranslate nohighlight">
\[
k(x, x') = \frac{1}{\Gamma(\nu) 2^{\nu - 1}} \left(\frac{\sqrt{2 \nu} |x - x'|}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2 \nu} |x - x'|}{\ell}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\( \nu \)</span> controls the smoothness, and <span class="math notranslate nohighlight">\( \ell \)</span> is the length scale parameter.</p>
<p><strong>Kernels</strong> in Gaussian Processes allow for great flexibility and power, enabling models to capture complex, nonlinear relationships in the data while quantifying uncertainty in predictions.</p>
</section>
</section>
<section id="apply-kernel-trick">
<h2>Apply Kernel Trick<a class="headerlink" href="#apply-kernel-trick" title="Link to this heading">#</a></h2>
<p>For a GP, the covariance between the training points <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> and test point <span class="math notranslate nohighlight">\( \mathbf{x_*} \)</span> is determined by the kernel function:</p>
<ol class="arabic simple">
<li><p><strong>Covariance matrix between training points</strong>:
$<span class="math notranslate nohighlight">\(
K(\mathbf{X}, \mathbf{X}) = \Phi(\mathbf{X}) \Phi(\mathbf{X})^\top
\)</span><span class="math notranslate nohighlight">\(
where \)</span> \Phi(\mathbf{X}) <span class="math notranslate nohighlight">\( is the feature map associated with the data points in \)</span> \mathbf{X} $.</p></li>
<li><p><strong>Covariance between training and test points</strong>:
$<span class="math notranslate nohighlight">\(
K(\mathbf{X}, \mathbf{x_*}) = \Phi(\mathbf{X}) \Phi(\mathbf{x_*})^\top
\)</span><span class="math notranslate nohighlight">\(
where \)</span> \Phi(\mathbf{x_<em>}) <span class="math notranslate nohighlight">\( is the feature map for the test point \)</span> \mathbf{x_</em>} $.</p></li>
<li><p><strong>Covariance matrix of the test points</strong>:
$<span class="math notranslate nohighlight">\(
K(\mathbf{x_*}, \mathbf{x_*}) = \Phi(\mathbf{x_*}) \Phi(\mathbf{x_*})^\top
\)</span>$</p></li>
</ol>
<p>By using the kernel trick, you don’t need to explicitly compute <span class="math notranslate nohighlight">\( \Phi(\mathbf{X}) \)</span> or <span class="math notranslate nohighlight">\( \Phi(\mathbf{x_*}) \)</span>. Instead, you calculate the kernel values directly, allowing the GP model to make predictions based on these kernel values without needing to transform the data.</p>
<section id="step-1-recall-the-prior-and-likelihood">
<h3>Step 1: Recall the prior and likelihood<a class="headerlink" href="#step-1-recall-the-prior-and-likelihood" title="Link to this heading">#</a></h3>
<p>The prior for the function values is a Gaussian process:</p>
<div class="math notranslate nohighlight">
\[
f \sim \mathcal{N}(0, K)
\]</div>
<p>where <span class="math notranslate nohighlight">\( K \)</span> is the covariance matrix given by the kernel function <span class="math notranslate nohighlight">\( k(x, x') = \phi(x)^\top \phi(x') \)</span>.</p>
<p>The likelihood is:</p>
<div class="math notranslate nohighlight">
\[
y = f + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma_n^2 I)
\]</div>
<p>where <span class="math notranslate nohighlight">\( y \)</span> are the observed outputs, and <span class="math notranslate nohighlight">\( \sigma_n^2 \)</span> is the noise variance.</p>
<p>Given the matrix:</p>
<div class="math notranslate nohighlight">
\[
A = \sigma_n^{-2} \Phi \Phi^\top + \Sigma_p^{-1}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \sigma_n^2 \)</span> is the variance of the noise,</p></li>
<li><p><span class="math notranslate nohighlight">\( \Phi \)</span> is the design matrix (which typically contains the features of the input data),</p></li>
<li><p><span class="math notranslate nohighlight">\( \Sigma_p^{-1} \)</span> is the prior covariance matrix (typically the inverse of the prior covariance matrix).</p></li>
</ul>
<p>To compute <span class="math notranslate nohighlight">\( A^{-1} \)</span>, we can use a known matrix identity for the inverse of a sum of two matrices:</p>
<div class="math notranslate nohighlight">
\[
(A + B)^{-1} = A^{-1} - A^{-1} B (I + B A^{-1})^{-1} A^{-1}
\]</div>
<p>However, since <span class="math notranslate nohighlight">\( A \)</span> is a sum of two matrices where <span class="math notranslate nohighlight">\( A = \sigma_n^{-2} \Phi \Phi^\top + \Sigma_p^{-1} \)</span>, we can apply the formula directly by noting the specific structure of <span class="math notranslate nohighlight">\( A \)</span>.</p>
<p>Now, applying the <strong>Woodbury matrix identity</strong>, which is a generalization of the matrix inversion lemma. This identity is helpful when you have a matrix of the form <span class="math notranslate nohighlight">\( A = U V^\top + C \)</span>, where <span class="math notranslate nohighlight">\( U \)</span> and <span class="math notranslate nohighlight">\( V \)</span> are matrices and <span class="math notranslate nohighlight">\( C \)</span> is another matrix.</p>
<p>The <strong>Woodbury identity</strong> states that:</p>
<div class="math notranslate nohighlight">
\[
(A + U V^\top)^{-1} = A^{-1} - A^{-1} U (I + V^\top A^{-1} U)^{-1} V^\top A^{-1}
\]</div>
<p><strong>Proof:</strong></p>
<p>The <strong>Woodbury matrix identity</strong> is given by:</p>
<div class="math notranslate nohighlight">
\[
(M + U V^\top)^{-1} = M^{-1} - M^{-1} U \left(I + V^\top M^{-1} U\right)^{-1} V^\top M^{-1}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( M \)</span> is an <span class="math notranslate nohighlight">\( m \times m \)</span> invertible matrix,</p></li>
<li><p><span class="math notranslate nohighlight">\( U \)</span> is an <span class="math notranslate nohighlight">\( m \times k \)</span> matrix,</p></li>
<li><p><span class="math notranslate nohighlight">\( V \)</span> is a <span class="math notranslate nohighlight">\( k \times m \)</span> matrix,</p></li>
<li><p><span class="math notranslate nohighlight">\( I \)</span> is the identity matrix of size <span class="math notranslate nohighlight">\( k \times k \)</span>.</p></li>
</ul>
<p><strong>Start with the expression for the inverse</strong></p>
<p>Let <span class="math notranslate nohighlight">\( B = M + U V^\top \)</span>, and suppose we want to find <span class="math notranslate nohighlight">\( B^{-1} \)</span>, so that:</p>
<div class="math notranslate nohighlight">
\[
B B^{-1} = I
\]</div>
<p>This means:</p>
<div class="math notranslate nohighlight">
\[
(M + U V^\top)(B^{-1}) = I
\]</div>
<p><strong>Assume the form of the inverse</strong></p>
<p>We hypothesize that <span class="math notranslate nohighlight">\( B^{-1} \)</span> has the form:</p>
<div class="math notranslate nohighlight">
\[
B^{-1} = M^{-1} - M^{-1} U X V^\top M^{-1}
\]</div>
<p>where <span class="math notranslate nohighlight">\( X \)</span> is to be determined.</p>
<p><strong>Substitute into the identity</strong></p>
<p>Now, substitute this assumption into the equation:</p>
<div class="math notranslate nohighlight">
\[
(M + U V^\top)(M^{-1} - M^{-1} U X V^\top M^{-1}) = I
\]</div>
<p><strong>Expand the product</strong></p>
<p>Expanding the terms gives:</p>
<div class="math notranslate nohighlight">
\[
M M^{-1} - M M^{-1} U X V^\top M^{-1} + U V^\top M^{-1} - U V^\top M^{-1} U X V^\top M^{-1}
\]</div>
<p>Simplifying:</p>
<div class="math notranslate nohighlight">
\[
I - M^{-1} U X V^\top M^{-1} + U V^\top M^{-1} - U V^\top M^{-1} U X V^\top M^{-1}
\]</div>
<p><strong>Solve for <span class="math notranslate nohighlight">\( X \)</span></strong></p>
<p>In order to match the identity <span class="math notranslate nohighlight">\( I \)</span>, we need to set the correction terms equal to zero. This gives the condition:</p>
<div class="math notranslate nohighlight">
\[
X = (I + V^\top M^{-1} U)^{-1}
\]</div>
<p><strong>Final Expression for the Inverse</strong></p>
<p>Therefore, the inverse of <span class="math notranslate nohighlight">\( M + U V^\top \)</span> is:</p>
<div class="math notranslate nohighlight">
\[
(M + U V^\top)^{-1} = M^{-1} - M^{-1} U \left(I + V^\top M^{-1} U\right)^{-1} V^\top M^{-1}
\]</div>
<p>In our case, <span class="math notranslate nohighlight">\( A =\Sigma_p^{-1} + \sigma_n^{-2} \Phi \Phi^\top \)</span>, so we can apply the identity here:</p>
<div class="math notranslate nohighlight">
\[
A^{-1} = \left( \Sigma_p^{-1} + \sigma_n^{-2} \Phi \Phi^\top \right)^{-1}
\]</div>
<p>Using the Woodbury identity, we have:</p>
<div class="math notranslate nohighlight">
\[
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
\]</div>
<p>Thus, the inverse of <span class="math notranslate nohighlight">\( A \)</span> is:</p>
<div class="math notranslate nohighlight">
\[
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
\]</div>
<p>This formula is crucial for solving for the posterior covariance in <strong>Bayesian linear regression</strong> or <strong>Gaussian Process regression</strong>, and involves the following components:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \Sigma_p \)</span> is the prior covariance matrix (usually known),</p></li>
<li><p><span class="math notranslate nohighlight">\( \Phi \)</span> is the design matrix (containing the features of your input data),</p></li>
<li><p><span class="math notranslate nohighlight">\( \sigma_n^2 \)</span> is the noise variance,</p></li>
<li><p>The term <span class="math notranslate nohighlight">\( \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \)</span> is a key part of the computation.</p></li>
</ul>
</section>
<section id="step-2-posterior-distribution">
<h3>Step 2: Posterior Distribution<a class="headerlink" href="#step-2-posterior-distribution" title="Link to this heading">#</a></h3>
<p>The posterior distribution of <span class="math notranslate nohighlight">\( f_* \)</span> given <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( y \)</span> is:</p>
<div class="math notranslate nohighlight">
\[
f_* | x_*, X, y \sim \mathcal{N}(\mu_*, \Sigma_*)
\]</div>
<p>where the posterior mean <span class="math notranslate nohighlight">\( \mu_* \)</span> and covariance <span class="math notranslate nohighlight">\( \Sigma_* \)</span> are:</p>
<ul class="simple">
<li><p><strong>Mean:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mu_* = \frac{1}{\sigma_n^2} \phi(x_*)^\top A^{-1} \phi^\top y
\]</div>
<ul class="simple">
<li><p><strong>Variance:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sigma_* = \phi(x_*)^\top A^{-1} \phi(x_*)
\]</div>
</section>
<section id="step-3-expressing-in-kernel-form">
<h3>Step 3: Expressing in Kernel Form<a class="headerlink" href="#step-3-expressing-in-kernel-form" title="Link to this heading">#</a></h3>
<p>Now, we will express these in terms of kernel functions.</p>
<section id="posterior-mean-in-kernel-form">
<h4>3.1 Posterior Mean in Kernel Form<a class="headerlink" href="#posterior-mean-in-kernel-form" title="Link to this heading">#</a></h4>
<p>The posterior mean <span class="math notranslate nohighlight">\( \mu_* \)</span> can be written as:</p>
<p>We start with the equation for <span class="math notranslate nohighlight">\( A^{-1} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
\]</div>
<p>Substitute this expression into the formula for the posterior mean:</p>
<div class="math notranslate nohighlight">
\[
\mu_* = \frac{1}{\sigma_n^2} \phi(x_*)^\top A^{-1} \phi^\top y
\]</div>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[
\sigma_n^{-2} \phi^\top (K+\sigma^2I)
\]</div>
<p>As <span class="math notranslate nohighlight">\(K=\phi^\top\Sigma_p\phi\)</span>, we have:
$<span class="math notranslate nohighlight">\(
\sigma_n^{-2} \phi^\top (\phi^\top\Sigma_p\phi+\sigma^2I)
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
\sigma_n^{-2} \phi^\top \phi \Sigma_p \phi^\top+\phi^\top
\]</div>
<p>Then we factor <span class="math notranslate nohighlight">\(\Sigma_p \phi^\top\)</span> from the right.
$<span class="math notranslate nohighlight">\(
(\sigma_n^{-2}\phi^\top\phi+\Sigma_p^{-1})\Sigma_p\phi^\top
\)</span>$</p>
<p>As <span class="math notranslate nohighlight">\(A=\sigma_n^{-2}\phi^\top\phi+\Sigma_p^{-1}\)</span> then we have:</p>
<div class="math notranslate nohighlight">
\[
\sigma_n^{-2}\phi^\top(K+\sigma^2I)=A\Sigma_p\phi^\top
\]</div>
<p>Now we multiply both side to <span class="math notranslate nohighlight">\(A^-1\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_n^{-2}A^{-1}\phi^\top(K+\sigma^2I)=\Sigma_p\phi^\top
\]</div>
<p>Now we multiply both side from right to <span class="math notranslate nohighlight">\((K+\sigma^2I)\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_n^{-2}A^{-1}\phi^\top=\Sigma_p\phi^\top(K+\sigma^2I)^{-1}
\]</div>
<p>Then As we know that <span class="math notranslate nohighlight">\(\mu_*=\sigma_n^{-2}\phi^\top(x_*)A^{-1}\phi^\top y\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\mu_*=\phi(x_*)^\top \Sigma_p \phi^\top (K+\sigma^2I)^{-1}y
\]</div>
</section>
<section id="posterior-variance-in-kernel-form">
<h4>3.2 Posterior Variance in Kernel Form<a class="headerlink" href="#posterior-variance-in-kernel-form" title="Link to this heading">#</a></h4>
<p>The posterior covariance <span class="math notranslate nohighlight">\( \Sigma_* \)</span> can be written as:
We start with the expression for the posterior covariance:</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(f_* | x_*, X, y) = K(x_*, x_*) - K(x_*, X)^\top A^{-1} K(X, x_*)
\]</div>
<p>Next, substitute the expression for <span class="math notranslate nohighlight">\( A^{-1} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
\]</div>
<p>This gives:</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(f_* | x_*, X, y) = K(x_*, x_*) - K(x_*, X)^\top \left[ \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p \right] K(X, x_*)
\]</div>
<p>Expanding the terms inside the brackets:</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(f_* | x_*, X, y) = K(x_*, x_*) - K(x_*, X)^\top \Sigma_p K(X, x_*) + K(x_*, X)^\top \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p K(X, x_*)
\]</div>
<p>Now, express in terms of kernels:</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(f_* | x_*, X, y) = k(x_*, x_*) - k(x_*, X)^\top K(X, X)^{-1} k(X, x_*) + k(x_*, X)^\top K(X, X)^{-1} \left( \sigma_n^2 + k(X, X) \right)^{-1} K(X, X)^{-1} k(X, x_*)
\]</div>
<p>The final expression for the posterior covariance is:</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(f_* | x_*, X, y) = k(x_*, x_*) - k(x_*, X)^\top K(X, X)^{-1} k(X, x_*)
\]</div>
</section>
</section>
<section id="final-result">
<h3>Final Result<a class="headerlink" href="#final-result" title="Link to this heading">#</a></h3>
<p>Thus, the posterior mean and covariance are:</p>
<ul class="simple">
<li><p><strong>Mean:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mu_* = k(x_*, X)^\top K(X, X)^{-1} y
\]</div>
<ul class="simple">
<li><p><strong>Covariance:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\Sigma_* = k(x_*, x_*) - k(x_*, X)^\top K(X, X)^{-1} k(X, x_*)
\]</div>
<p><strong>Final Formula:</strong></p>
<p>Thus, the final posterior distribution of <span class="math notranslate nohighlight">\( f_* \)</span> is:</p>
<div class="math notranslate nohighlight">
\[
f_* | \mathbf{x_*}, \mathbf{X}, \mathbf{y} \sim \mathcal{N}\left( \phi_*^\top \Sigma_p \Phi (K + \sigma_n^2 I)^{-1} \mathbf{y}, \phi_*^\top \Sigma_p \phi_* - \phi_*^\top \Sigma_p \Phi (K + \sigma_n^2 I)^{-1} \Phi^\top \Sigma_p \phi_* \right)
\]</div>
<p><strong><strong>Explanation of Key Components:</strong></strong></p>
<ul class="simple">
<li><p><strong>Feature Map</strong> <span class="math notranslate nohighlight">\( \phi_* \)</span> and <span class="math notranslate nohighlight">\( \Phi \)</span>: The feature map <span class="math notranslate nohighlight">\( \phi(\cdot) \)</span> transforms the data into a higher-dimensional space, often infinite-dimensional, to capture the non-linearity of the data. In this equation, <span class="math notranslate nohighlight">\( \phi_* \)</span> refers to the feature map for the test point, and <span class="math notranslate nohighlight">\( \Phi \)</span> is the matrix of feature maps for the training data.</p></li>
<li><p><strong>Prior Covariance</strong> <span class="math notranslate nohighlight">\( \Sigma_p \)</span>: This represents the prior belief about the function’s behavior before seeing any data. It’s typically chosen to reflect properties like smoothness or periodicity.</p></li>
<li><p><strong>Kernel Matrix</strong> <span class="math notranslate nohighlight">\( K \)</span>: The kernel matrix captures the similarities between training points and is used to define the covariance between function values at those points. The kernel trick allows us to compute the similarities in the feature space without explicitly computing <span class="math notranslate nohighlight">\( \phi(\cdot) \)</span>.</p></li>
<li><p><strong>Noise</strong> <span class="math notranslate nohighlight">\( \sigma_n^2 \)</span>: The noise term accounts for the observational noise in the training data.</p></li>
</ul>
</section>
</section>
<section id="function-space-view-non-parametric-in-gaussian-processes">
<h2>Function-Space View (Non parametric) in Gaussian Processes<a class="headerlink" href="#function-space-view-non-parametric-in-gaussian-processes" title="Link to this heading">#</a></h2>
<p>A <strong>Gaussian Process (GP)</strong> is a powerful and flexible approach for modeling and making predictions about functions. Instead of directly estimating a parameterized model (e.g., linear regression or neural networks), a GP defines a <strong>distribution over functions</strong>. This allows us to treat functions as random variables and make predictions with an associated uncertainty, rather than a single deterministic outcome. In essence, the GP framework enables us to work directly in the <strong>function space</strong> by providing a distribution over all possible functions that are consistent with the observed data.</p>
<p>In the <strong>function-space view</strong>, we consider the following key concepts:</p>
<section id="prior-over-functions">
<h3>1. Prior Over Functions<a class="headerlink" href="#prior-over-functions" title="Link to this heading">#</a></h3>
<p>Before observing any data, we assume that the function values at any collection of input points come from a multivariate Gaussian distribution. This is referred to as the <strong>prior distribution</strong> over functions. For a set of input points <span class="math notranslate nohighlight">\(( x_1, x_2, \dots, x_n )\)</span>, the values of the function at these points, denoted as <span class="math notranslate nohighlight">\(( f(x_1), f(x_2), \dots, f(x_n) )\)</span>, follow a joint Gaussian distribution with mean and covariance given by the following:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f(x)} \sim \mathcal{GP}(\mathbf{m(x)}, \mathbf{K(x,x')})
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n)
\end{bmatrix}
\sim \mathcal{N}\left(
\begin{bmatrix}
m(x_1) \\
m(x_2) \\
\vdots \\
m(x_n)
\end{bmatrix},
\begin{bmatrix}
K(x_1, x_1) &amp; K(x_1, x_2) &amp; \cdots &amp; K(x_1, x_n) \\
K(x_2, x_1) &amp; K(x_2, x_2) &amp; \cdots &amp; K(x_2, x_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
K(x_n, x_1) &amp; K(x_n, x_2) &amp; \cdots &amp; K(x_n, x_n)
\end{bmatrix}
\right)
\end{split}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><strong>Mean vector</strong> <span class="math notranslate nohighlight">\((\mathbf{m} = [m(x_1), m(x_2), \dots, m(x_n)])\)</span>, typically assumed to be zero unless specified otherwise. The mean function represents the expected value of the function at each input point.</p></li>
<li><p><strong>Covariance matrix</strong> <span class="math notranslate nohighlight">\((\mathbf{K})\)</span>, whose elements are computed by the kernel function <span class="math notranslate nohighlight">\(( k(x_i, x_j) )\)</span>. The kernel function defines how related the function values at different points are, i.e., it controls the smoothness, periodicity, and other properties of the function.</p></li>
</ul>
<p>In a Gaussian Process, the kernel function is a crucial component that encodes the assumptions about the smoothness and variability of the underlying function.</p>
</section>
<section id="joint-normal-distribution">
<h3>2. Joint Normal Distribution<a class="headerlink" href="#joint-normal-distribution" title="Link to this heading">#</a></h3>
<p>The <strong>joint normal distribution</strong> describes the behavior of a set of random variables <span class="math notranslate nohighlight">\(( X_1, X_2, \ldots, X_n )\)</span> that are jointly Gaussian. This means every linear combination of these variables is also normally distributed.</p>
<section id="definition">
<h4>Definition<a class="headerlink" href="#definition" title="Link to this heading">#</a></h4>
<p>If <span class="math notranslate nohighlight">\(( \mathbf{X} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix} )\)</span> is an <span class="math notranslate nohighlight">\(( n )\)</span>-dimensional random vector, it follows a joint normal distribution if:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} \sim \mathcal{N}\left( 
\boldsymbol{\mu}, \mathbf{\Sigma}
\right),
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(( \boldsymbol{\mu} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix} )\)</span> is the mean vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(( \mathbf{\Sigma} = \begin{bmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1n} \\
\sigma_{21} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{n1} &amp; \sigma_{n2} &amp; \cdots &amp; \sigma_{nn}
\end{bmatrix} )\)</span> is the covariance matrix, where <span class="math notranslate nohighlight">\(( \sigma_{ij} = \text{Cov}(X_i, X_j) )\)</span>.</p></li>
</ul>
</section>
<section id="joint-pdf">
<h4>Joint PDF<a class="headerlink" href="#joint-pdf" title="Link to this heading">#</a></h4>
<p>The probability density function (PDF) is:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{X}) = \frac{1}{(2\pi)^{n/2} |\mathbf{\Sigma}|^{1/2}} 
\exp\left( -\frac{1}{2} (\mathbf{X} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) \right),
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(( |\mathbf{\Sigma}| )\)</span> is the determinant of the covariance matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(( \mathbf{\Sigma}^{-1} )\)</span> is the inverse of the covariance matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(( (\mathbf{X} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) )\)</span> is the <strong>Mahalanobis distance</strong>.</p></li>
</ul>
<p>This distribution is used widely in multivariate statistics, machine learning, and Gaussian processes.</p>
</section>
<section id="marginal-distribution">
<h4>Marginal Distribution<a class="headerlink" href="#marginal-distribution" title="Link to this heading">#</a></h4>
<p>The <strong>marginal distribution</strong> of a subset of a multivariate normal distribution is also a multivariate normal distribution. Suppose <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is partitioned into two sets <span class="math notranslate nohighlight">\(\mathbf{X}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} = \begin{bmatrix} \mathbf{X}_1 \\ \mathbf{X}_2 \end{bmatrix}, \quad \boldsymbol{\mu} = \begin{bmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{bmatrix}, \quad \mathbf{\Sigma} = \begin{bmatrix} \mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\ \mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22} \end{bmatrix}
\end{split}\]</div>
<p>Then the marginal distribution of <span class="math notranslate nohighlight">\(\mathbf{X}_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}_1 \sim \mathcal{N}(\boldsymbol{\mu}_1, \mathbf{\Sigma}_{11})
\]</div>
</section>
<section id="conditional-distribution">
<h4>Conditional Distribution<a class="headerlink" href="#conditional-distribution" title="Link to this heading">#</a></h4>
<p>The <strong>conditional distribution</strong> of one subset of a multivariate normal distribution given another subset is also a multivariate normal distribution. Suppose <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is partitioned as above. The conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{X}_1\)</span> given <span class="math notranslate nohighlight">\(\mathbf{X}_2 = \mathbf{x}_2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}_1 | \mathbf{X}_2 = \mathbf{x}_2 \sim \mathcal{N}(\boldsymbol{\mu}_{1|2}, \mathbf{\Sigma}_{1|2}),
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{1|2} = \boldsymbol{\mu}_1 + \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1} (\mathbf{x}_2 - \boldsymbol{\mu}_2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{\Sigma}_{1|2} = \mathbf{\Sigma}_{11} - \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}\mathbf{\Sigma}_{21}\)</span></p></li>
</ul>
</section>
</section>
<section id="posterior-over-functions">
<h3>3. Posterior Over Functions<a class="headerlink" href="#posterior-over-functions" title="Link to this heading">#</a></h3>
<p>For simplicity, the mean function is often taken to be zero:</p>
<div class="math notranslate nohighlight">
\[m(x) = 0\]</div>
<div class="math notranslate nohighlight">
\[
\mathbf{f(x)} \sim \mathcal{GP}(0, \mathbf{K(x,x')})
\]</div>
<div class="math notranslate nohighlight">
\[\text{cov}(f(x_p), f(x_q)) = k(x_p, x_q) = \mathbb{E}[(f(x)-m(x))(f(x') - m(x'))] = \mathbb{E}[f(x)f(x')]  \]</div>
<p>Let’s assume that we don’t have noise at the beginning (noise-free observation), so we have a series of <span class="math notranslate nohighlight">\((x_1,x_2,...,x_n)\)</span> and we have a <span class="math notranslate nohighlight">\((f(x_1),f(x_2),...,f(x_n))\)</span>, now a new data is entered <span class="math notranslate nohighlight">\((x_*)\)</span>, the goal is to find the distribution of <span class="math notranslate nohighlight">\((f(x_*))\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n) \\
f(x_*)
\end{bmatrix}
\sim \mathcal{N}\left(
0,
\begin{bmatrix}
K(X,X) &amp; K(X,X_*)  \\
K(X_*,X) &amp; K(X_*,X_*) \\
\end{bmatrix}
\right)
\end{split}\]</div>
<p>Now, if we open the expression of normal distribution and write it as a formula, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\exp\left(-\frac{1}{2} 
\begin{bmatrix} 
f \\ 
f_* 
\end{bmatrix}^\top 
\mathbf{\Sigma}^{-1} 
\begin{bmatrix} 
f \\ 
f_* 
\end{bmatrix} 
\right)
\end{split}\]</div>
<p>Using the concept of conditioning from the joint normal distribution, we can directly apply the formulas for the conditional mean and covariance. Given the joint distribution of <span class="math notranslate nohighlight">\( \begin{bmatrix} f \\ f_* \end{bmatrix} \)</span> with mean <span class="math notranslate nohighlight">\( \begin{bmatrix} \mu \\ \mu_* \end{bmatrix} \)</span> and covariance matrix <span class="math notranslate nohighlight">\( \mathbf{\Sigma} \)</span>, the conditional distribution of <span class="math notranslate nohighlight">\( f_* \)</span> given <span class="math notranslate nohighlight">\( f \)</span> is:</p>
<ol class="arabic">
<li><p><strong>Mean Calculation</strong>:
$<span class="math notranslate nohighlight">\(
\bar{f_*} = \mu_* + \mathbf{\Sigma}_{f_*f} \mathbf{\Sigma}_{ff}^{-1} (f - \mu)
\)</span>$</p>
<p>because we assume mean is zero, we have:
$<span class="math notranslate nohighlight">\(
\bar{f_*} = k(X_*, X)K(X, X)^{-1}f
\)</span>$</p>
</li>
<li><p><strong>Covariance Calculation</strong>:
$<span class="math notranslate nohighlight">\(
\mathbf{\Sigma}_{f_*|f} = \mathbf{\Sigma}_{f_*f_*} - \mathbf{\Sigma}_{f_*f} \mathbf{\Sigma}_{ff}^{-1} \mathbf{\Sigma}_{ff_*}
\)</span>$</p>
<p>Here, it translates to:
$<span class="math notranslate nohighlight">\(
\mathbf{\Sigma} = \text{cov}(f_*) = k(X_*, X_*) - k(X_*, X)^\top K(X, X)^{-1} k(X, X_*)
\)</span>$</p>
</li>
</ol>
<p>Thus, the conditional mean and covariance of ( f_* ) given ( f ) are derived as shown above, based on the conditionalization formulas in the joint normal distribution.</p>
</section>
<section id="incorporate-observational-noise">
<h3>Incorporate Observational Noise<a class="headerlink" href="#incorporate-observational-noise" title="Link to this heading">#</a></h3>
<p>Assume we have noisy observations:
$<span class="math notranslate nohighlight">\( y_i = f(x_i) + \epsilon_i \)</span>$</p>
<ul class="simple">
<li><p>Where <span class="math notranslate nohighlight">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>.</p></li>
</ul>
<p>Now we have:
$<span class="math notranslate nohighlight">\( y_i | f(x_i) \sim \mathcal{N}(f(x_i), \sigma^2) \)</span>$</p>
<p>Consider the function values as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
F = 
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n)
\end{bmatrix}  
\sim \mathcal{N}(0,K(x,x'))
\end{split}\]</div>
<ul class="simple">
<li><p>Where <span class="math notranslate nohighlight">\( k(x_i,x_j) = Cov(f(x_i), f(x_j)) \)</span></p></li>
</ul>
<p>Now, suppose we have noisy observation for <span class="math notranslate nohighlight">\( Y \)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n 
\end{bmatrix} 
\end{split}\]</div>
<p>The joint distribution of the noisy observation for <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
Y = F + \epsilon
\]</div>
<p>Thus, the noise variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is added to the diagonal of the covariance matrix to have the joint distiribution for <span class="math notranslate nohighlight">\( Y \)</span>:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{Y} \sim \mathcal{N}(\mathbf{0}, \mathbf{K} + \sigma^2 \mathbf{I}) \]</div>
<p>Now suppose we have observational noise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n) \\
f(x_*)
\end{bmatrix}
\sim \mathcal{N}\left(
0,
\begin{bmatrix}
K(X,X)+ \sigma^2 \mathbf{I} &amp; K(X,X_*)  \\
K(X_*,X) &amp; K(X_*,X_*) \\
\end{bmatrix}
\right)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[ \bar{f_*} = k(X_*, X)(K(X, X)+\sigma_n^2 \mathbf{I})^{-1}f\]</div>
<div class="math notranslate nohighlight">
\[ \mathbf{\Sigma}=\text{cov}(f_*) = k(X_*, X_*) - k(X_*, X)^\top (K(X, X)+\sigma_n^2 \mathbf{I})^{-1} k(X, X_*) \]</div>
</section>
</section>
<section id="numerical-example-of-gaussian-process-regression">
<h2>Numerical Example of Gaussian Process Regression<a class="headerlink" href="#numerical-example-of-gaussian-process-regression" title="Link to this heading">#</a></h2>
<section id="problem-setup">
<h3>Problem Setup<a class="headerlink" href="#problem-setup" title="Link to this heading">#</a></h3>
<p>We are tasked with modeling a function ( f(x) ) using Gaussian Process Regression (GPR). The data, kernel, and parameters are as follows:</p>
<ul class="simple">
<li><p><strong>Training data</strong>:</p>
<ul>
<li><p>Inputs: <span class="math notranslate nohighlight">\(( X_{\text{train}} = \{-3, -2, -1, 0, 1, 2, 3\} )\)</span></p></li>
<li><p>Observations: <span class="math notranslate nohighlight">\(( y_{\text{train}} = \{2.5, 1.8, 1.2, 0.5, -0.2, -1.2, -2.0\} )\)</span></p></li>
</ul>
</li>
<li><p><strong>Kernel</strong>: Radial Basis Function (RBF) kernel:
$<span class="math notranslate nohighlight">\(
k(x_i, x_j) = \sigma_f^2 \exp\left(-\frac{\|x_i - x_j\|^2}{2\ell^2}\right)
\)</span>$</p>
<ul>
<li><p>Signal variance <span class="math notranslate nohighlight">\(( \sigma_f^2 )\)</span> = 1.0</p></li>
<li><p>Length scale <span class="math notranslate nohighlight">\(( \ell )\)</span> = 1.0</p></li>
</ul>
</li>
<li><p><strong>Noise variance <span class="math notranslate nohighlight">\(( \sigma_n^2 )\)</span></strong> = 0.1</p></li>
<li><p><strong>Test points</strong>: <span class="math notranslate nohighlight">\(( X_{\text{test}} = \{-4, -3, -2, -1, 0, 1, 2, 3, 4\} )\)</span></p></li>
</ul>
</section>
<section id="step-1-compute-covariance-matrices">
<h3>Step 1: Compute Covariance Matrices<a class="headerlink" href="#step-1-compute-covariance-matrices" title="Link to this heading">#</a></h3>
<section id="training-covariance-matrix-mathbf-k">
<h4>1. Training covariance matrix <span class="math notranslate nohighlight">\(( \mathbf{K} )\)</span><a class="headerlink" href="#training-covariance-matrix-mathbf-k" title="Link to this heading">#</a></h4>
<p>For the training data points <span class="math notranslate nohighlight">\(( X_{\text{train}} )\)</span>, compute the pairwise covariance using the RBF kernel.</p>
<p>For example:
$<span class="math notranslate nohighlight">\(
k(-3, -3) = \sigma_f^2 \exp\left(-\frac{(-3 - (-3))^2}{2 \ell^2}\right) = 1.0
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
k(-3, -2) = \sigma_f^2 \exp\left(-\frac{(-3 - (-2))^2}{2 \ell^2}\right) = 1.0 \exp\left(-\frac{1}{2}\right) \approx 0.6065
\]</div>
<p>The full covariance matrix for <span class="math notranslate nohighlight">\(( X_{\text{train}} )\)</span> is:
$<span class="math notranslate nohighlight">\(
\mathbf{K} =
\begin{bmatrix}
1.0000 &amp; 0.6065 &amp; 0.1353 &amp; 0.0111 &amp; 0.0003 &amp; 0.0000 &amp; 0.0000 \\
0.6065 &amp; 1.0000 &amp; 0.6065 &amp; 0.1353 &amp; 0.0111 &amp; 0.0003 &amp; 0.0000 \\
0.1353 &amp; 0.6065 &amp; 1.0000 &amp; 0.6065 &amp; 0.1353 &amp; 0.0111 &amp; 0.0003 \\
0.0111 &amp; 0.1353 &amp; 0.6065 &amp; 1.0000 &amp; 0.6065 &amp; 0.1353 &amp; 0.0111 \\
0.0003 &amp; 0.0111 &amp; 0.1353 &amp; 0.6065 &amp; 1.0000 &amp; 0.6065 &amp; 0.1353 \\
0.0000 &amp; 0.0003 &amp; 0.0111 &amp; 0.1353 &amp; 0.6065 &amp; 1.0000 &amp; 0.6065 \\
0.0000 &amp; 0.0000 &amp; 0.0003 &amp; 0.0111 &amp; 0.1353 &amp; 0.6065 &amp; 1.0000
\end{bmatrix}
\)</span>$</p>
</section>
<section id="test-covariance-vector-mathbf-k">
<h4>2. Test covariance vector <span class="math notranslate nohighlight">\(( \mathbf{k}_* )\)</span><a class="headerlink" href="#test-covariance-vector-mathbf-k" title="Link to this heading">#</a></h4>
<p>For each test point <span class="math notranslate nohighlight">\(( x_* )\)</span>, compute the covariance with all training points using the kernel function.</p>
<section id="example-for-x-4">
<h5>Example for ( x_* = -4 ):<a class="headerlink" href="#example-for-x-4" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
k(-4, -3) = \sigma_f^2 \exp\left(-\frac{(-4 - (-3))^2}{2 \ell^2}\right) = 1.0 \exp\left(-\frac{1}{2}\right) \approx 0.6065
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbf{k}_* = [0.6065, 0.1353, 0.0111, 0.0003, 0.0000, 0.0000, 0.0000]
\]</div>
</section>
<section id="computation-for-all-test-points">
<h5>Computation for all test points:<a class="headerlink" href="#computation-for-all-test-points" title="Link to this heading">#</a></h5>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(( x_* )\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(( \mathbf{k}_* )\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>( -4 )</p></td>
<td><p>( [0.6065, 0.1353, 0.0111, 0.0003, 0.0000, 0.0000, 0.0000] )</p></td>
</tr>
<tr class="row-odd"><td><p>( -3 )</p></td>
<td><p>( [1.0000, 0.6065, 0.1353, 0.0111, 0.0003, 0.0000, 0.0000] )</p></td>
</tr>
<tr class="row-even"><td><p>( -2 )</p></td>
<td><p>( [0.6065, 1.0000, 0.6065, 0.1353, 0.0111, 0.0003, 0.0000] )</p></td>
</tr>
<tr class="row-odd"><td><p>( -1 )</p></td>
<td><p>( [0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0111, 0.0003] )</p></td>
</tr>
<tr class="row-even"><td><p>(  0 )</p></td>
<td><p>( [0.0111, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0111] )</p></td>
</tr>
<tr class="row-odd"><td><p>(  1 )</p></td>
<td><p>( [0.0003, 0.0111, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353] )</p></td>
</tr>
<tr class="row-even"><td><p>(  2 )</p></td>
<td><p>( [0.0000, 0.0003, 0.0111, 0.1353, 0.6065, 1.0000, 0.6065] )</p></td>
</tr>
<tr class="row-odd"><td><p>(  3 )</p></td>
<td><p>( [0.0000, 0.0000, 0.0003, 0.0111, 0.1353, 0.6065, 1.0000] )</p></td>
</tr>
<tr class="row-even"><td><p>(  4 )</p></td>
<td><p>( [0.0000, 0.0000, 0.0000, 0.0003, 0.0111, 0.1353, 0.6065] )</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="noise-adjusted-covariance-matrix-mathbf-k-y">
<h4>3. Noise-adjusted covariance matrix <span class="math notranslate nohighlight">\(( \mathbf{K}_y )\)</span><a class="headerlink" href="#noise-adjusted-covariance-matrix-mathbf-k-y" title="Link to this heading">#</a></h4>
<p>Add the noise variance <span class="math notranslate nohighlight">\(( \sigma_n^2 )\)</span> to the diagonal of <span class="math notranslate nohighlight">\(( \mathbf{K} )\)</span>:
$<span class="math notranslate nohighlight">\(
\mathbf{K}_y = \mathbf{K} + \sigma_n^2 \mathbf{I}
\)</span>$</p>
</section>
</section>
<section id="step-2-predictive-mean-and-variance">
<h3>Step 2: Predictive Mean and Variance<a class="headerlink" href="#step-2-predictive-mean-and-variance" title="Link to this heading">#</a></h3>
<section id="predictive-mean-mu-x">
<h4>1. Predictive mean <span class="math notranslate nohighlight">\(( \mu(x_*) )\)</span><a class="headerlink" href="#predictive-mean-mu-x" title="Link to this heading">#</a></h4>
<p>Using:
$<span class="math notranslate nohighlight">\(
\mu(x_*) = \mathbf{k}_*^\top (\mathbf{K}_y)^{-1} \mathbf{y}
\)</span>$</p>
<section id="example">
<h5>Example:<a class="headerlink" href="#example" title="Link to this heading">#</a></h5>
<p>For <span class="math notranslate nohighlight">\(( x_* = -4 )\)</span>:</p>
<ul class="simple">
<li><p>Compute <span class="math notranslate nohighlight">\((\mathbf{K}_y)^{-1} ) (inverse of ( \mathbf{K}_y )\)</span></p></li>
<li><p>Compute the dot product <span class="math notranslate nohighlight">\(( \mathbf{k}_*^\top (\mathbf{K}_y)^{-1} \mathbf{y} )\)</span></p></li>
</ul>
</section>
</section>
<section id="predictive-variance-sigma-2-x">
<h4>2. Predictive variance <span class="math notranslate nohighlight">\(( \sigma^2(x_*) )\)</span><a class="headerlink" href="#predictive-variance-sigma-2-x" title="Link to this heading">#</a></h4>
<p>Using:
$<span class="math notranslate nohighlight">\(
\sigma^2(x_*) = k(x_*, x_*) - \mathbf{k}_*^\top (\mathbf{K}_y)^{-1} \mathbf{k}_*
\)</span>$</p>
<section id="id2">
<h5>Example:<a class="headerlink" href="#id2" title="Link to this heading">#</a></h5>
<p>For <span class="math notranslate nohighlight">\(( x_* = -4 )\)</span>, substitute the values and compute.</p>
</section>
</section>
</section>
<section id="step-3-predictions-for-all-test-points">
<h3>Step 3: Predictions for All Test Points<a class="headerlink" href="#step-3-predictions-for-all-test-points" title="Link to this heading">#</a></h3>
<p>Perform the same computations for all test points <span class="math notranslate nohighlight">\(( x_* = \{-4, -3, -2, -1, 0, 1, 2, 3, 4\} )\)</span>. Example results:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(( x_* )\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(( \mu(x_*) )\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(( \sigma(x_*) )\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(-4)</p></td>
<td><p>( 2.2 )</p></td>
<td><p>( 0.39 )</p></td>
</tr>
<tr class="row-odd"><td><p>(-3)</p></td>
<td><p>( 2.5 )</p></td>
<td><p>( 0.10 )</p></td>
</tr>
<tr class="row-even"><td><p>(-2)</p></td>
<td><p>( 1.8 )</p></td>
<td><p>( 0.09 )</p></td>
</tr>
<tr class="row-odd"><td><p>(-1)</p></td>
<td><p>( 1.2 )</p></td>
<td><p>( 0.09 )</p></td>
</tr>
<tr class="row-even"><td><p>( 0 )</p></td>
<td><p>( 0.5 )</p></td>
<td><p>( 0.09 )</p></td>
</tr>
<tr class="row-odd"><td><p>( 1 )</p></td>
<td><p>(-0.2 )</p></td>
<td><p>( 0.09 )</p></td>
</tr>
<tr class="row-even"><td><p>( 2 )</p></td>
<td><p>(-1.2 )</p></td>
<td><p>( 0.09 )</p></td>
</tr>
<tr class="row-odd"><td><p>( 3 )</p></td>
<td><p>(-2.0 )</p></td>
<td><p>( 0.10 )</p></td>
</tr>
<tr class="row-even"><td><p>( 4 )</p></td>
<td><p>(-2.3 )</p></td>
<td><p>( 0.39 )</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="step-4-confidence-intervals">
<h3>Step 4: Confidence Intervals<a class="headerlink" href="#step-4-confidence-intervals" title="Link to this heading">#</a></h3>
<p>The 95% confidence interval for each test point is given by:
$<span class="math notranslate nohighlight">\(
[\mu(x_*) - 1.96 \cdot \sigma(x_*), \mu(x_*) + 1.96 \cdot \sigma(x_*)]
\)</span>$</p>
<p>This formula shows that with 95% probability, the predicted real value is in the range μ(x∗)±1.96×σ(x∗).</p>
<section id="id3">
<h4>Example:<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<p>For <span class="math notranslate nohighlight">\(( x_* = -4 )\)</span>:</p>
<div class="math notranslate nohighlight">
\[
[2.2 - 1.96 \cdot 0.39, 2.2 + 1.96 \cdot 0.39] \approx [1.44, 2.96]
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the RBF kernel</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">signal_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">sqdist</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Training data</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Test points</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 10 test points</span>

<span class="c1"># Kernel and noise</span>
<span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">signal_variance</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">noise_variance</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Compute the covariance matrices</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">signal_variance</span><span class="p">)</span>
<span class="n">K_s</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">signal_variance</span><span class="p">)</span>
<span class="n">K_ss</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">signal_variance</span><span class="p">)</span>
<span class="n">K_y</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">noise_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

<span class="c1"># Posterior mean and covariance</span>
<span class="n">K_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">K_y</span><span class="p">)</span>
<span class="n">mu_s</span> <span class="o">=</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">K_inv</span> <span class="o">@</span> <span class="n">y_train</span>
<span class="n">cov_s</span> <span class="o">=</span> <span class="n">K_ss</span> <span class="o">-</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">K_inv</span> <span class="o">@</span> <span class="n">K_s</span>
<span class="n">std_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_s</span><span class="p">))</span>

<span class="c1"># Print the predictive values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictive Mean and 95% Confidence Interval at Test Points:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">mu_s</span><span class="p">,</span> <span class="n">std_s</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x = </span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">: mean = </span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, 95% CI = [</span><span class="si">{</span><span class="n">mean</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">std</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">mean</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">std</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># Plot the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">mu_s</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mean Prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">mu_s</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">std_s</span><span class="p">,</span> <span class="n">mu_s</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">std_s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Confidence Interval (95%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gaussian Process Regression&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictive Mean and 95% Confidence Interval at Test Points:
x = -4.00: mean = 1.332, 95% CI = [-0.193, 2.856]
x = -3.11: mean = 2.247, 95% CI = [1.621, 2.872]
x = -2.22: mean = 1.982, 95% CI = [1.423, 2.542]
x = -1.33: mean = 1.314, 95% CI = [0.766, 1.862]
x = -0.44: mean = 0.788, 95% CI = [0.240, 1.336]
x = 0.44: mean = 0.200, 95% CI = [-0.348, 0.748]
x = 1.33: mean = -0.487, 95% CI = [-1.034, 0.061]
x = 2.22: mean = -1.418, 95% CI = [-1.978, -0.859]
x = 3.11: mean = -1.806, 95% CI = [-2.431, -1.181]
x = 4.00: mean = -1.075, 95% CI = [-2.599, 0.450]
</pre></div>
</div>
<img alt="../../_images/7cc298c5091e98390e66920ab3ef0259ae0a30867bb70033509bb80775cd5297.png" src="../../_images/7cc298c5091e98390e66920ab3ef0259ae0a30867bb70033509bb80775cd5297.png" />
</div>
</div>
</section>
<section id="proof-of-conditional-distribution-formula">
<h4>Proof of Conditional Distribution Formula<a class="headerlink" href="#proof-of-conditional-distribution-formula" title="Link to this heading">#</a></h4>
<p>In above section, we claimed that:</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}_1|\mathbf{x}_2) = \mathcal{N}(\mathbf{x}_1 | \boldsymbol{\mu}_1 + \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2), \mathbf{\Sigma}_{11} - \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}\mathbf{\Sigma}_{21})
\]</div>
<p>We derive this result using Schur complements. Let us factor the joint <span class="math notranslate nohighlight">\(p(\mathbf{x}_1, \mathbf{x}_2)\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
p(\mathbf{x}_1, \mathbf{x}_2) \propto \exp\left\{ -\frac{1}{2} \begin{pmatrix} \mathbf{x}_1 - \boldsymbol{\mu}_1 \\ \mathbf{x}_2 - \boldsymbol{\mu}_2 \end{pmatrix}^T \begin{pmatrix} \mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\ \mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22} \end{pmatrix}^{-1} \begin{pmatrix} \mathbf{x}_1 - \boldsymbol{\mu}_1 \\ \mathbf{x}_2 - \boldsymbol{\mu}_2 \end{pmatrix} \right\}
\end{split}\]</div>
<p>Schur complements:
$<span class="math notranslate nohighlight">\(
\begin{align*}
\begin{pmatrix}
E &amp; F \\
G &amp; H
\end{pmatrix}^{-1}
&amp;=
\begin{pmatrix}
I &amp; 0 \\
-H^{-1}G &amp; I
\end{pmatrix}
\begin{pmatrix}
(M/H)^{-1} &amp; 0 \\
0 &amp; H^{-1}
\end{pmatrix}
\begin{pmatrix}
I &amp; -FH^{-1} \\
0 &amp; I
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
(M/H)^{-1} &amp; 0 \\
-H^{-1}G(M/H)^{-1} &amp; H^{-1}
\end{pmatrix}
\begin{pmatrix}
I &amp; -FH^{-1} \\
0 &amp; I
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
(M/H)^{-1} &amp; -(M/H)^{-1}FH^{-1} \\
-H^{-1}G(M/H)^{-1} &amp; H^{-1}+H^{-1}G(M/H)^{-1}FH^{-1}
\end{pmatrix}
\end{align*}
\)</span>$</p>
<p>Using Schur complements, the exponent becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(x_1, x_2) &amp;\propto \exp\left\{-\frac{1}{2}\begin{pmatrix} (x_1-\mu_1)^\top \\ (x_2-\mu_2)^\top \end{pmatrix} \begin{pmatrix} I &amp; 0 \\ -\Sigma_{22}^{-1}\Sigma_{21} &amp; I \end{pmatrix} \begin{pmatrix} (\Sigma/\Sigma_{22})^{-1} &amp; 0 \\ 0 &amp; \Sigma_{22}^{-1} \end{pmatrix} \begin{pmatrix} I &amp; -\Sigma_{12}\Sigma_{22}^{-1} \\ 0 &amp; I \end{pmatrix} \begin{pmatrix} (x_1-\mu_1) \\ (x_2-\mu_2) \end{pmatrix} \right\} \\
&amp;\propto \exp\left\{-\frac{1}{2}\begin{pmatrix} (x_1-\mu_1)^\top &amp; (x_2-\mu_2)^\top \end{pmatrix} \begin{pmatrix} (\Sigma/\Sigma_{22})^{-1} &amp; -\Sigma_{12}\Sigma_{22}^{-1} \\ -\Sigma_{22}^{-1}\Sigma_{21} &amp; \Sigma_{22}^{-1} \end{pmatrix} \begin{pmatrix} (x_1-\mu_1) \\ (x_2-\mu_2) \end{pmatrix} \right\} \\
&amp;= \exp\left\{-\frac{1}{2}\left[(x_1-\mu_1)^\top (\Sigma/\Sigma_{22})^{-1} (x_1-\mu_1) - 2(x_1-\mu_1)^\top \Sigma_{12}\Sigma_{22}^{-1} (x_2-\mu_2) + (x_2-\mu_2)^\top \Sigma_{22}^{-1} (x_2-\mu_2)\right]\right\} \\
&amp;= \exp\left\{-\frac{1}{2}(x_1-\mu_1)^\top (\Sigma/\Sigma_{22})^{-1} (x_1-\mu_1)\right\} \times \exp\left\{-\frac{1}{2}(x_2-\mu_2)^\top \Sigma_{22}^{-1} (x_2-\mu_2)\right\}
\end{align*}
\end{split}\]</div>
<p>This is of the form:</p>
<div class="math notranslate nohighlight">
\[
\exp(\text{quadratic form in } \mathbf{x}_1, \mathbf{x}_2) \times \exp(\text{quadratic form in } \mathbf{x}_2)
\]</div>
<p>Hence, we have successfully factorized the joint as:</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{x}_1, \mathbf{x}_2) = p(\mathbf{x}_2)p(\mathbf{x}_1|\mathbf{x}_2)
\]</div>
<div class="math notranslate nohighlight">
\[
= \mathcal{N}(\mathbf{x}_1|\boldsymbol{\mu}_{1|2}, \mathbf{\Sigma}_{1|2})\mathcal{N}(\mathbf{x}_2|\boldsymbol{\mu}_2, \mathbf{\Sigma}_{22}),
\]</div>
<p>where the parameters of the conditional distribution can be read off from the above equations using:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_{1|2} = \boldsymbol{\mu}_1 + \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2)
\]</div>
<div class="math notranslate nohighlight">
\[
\mathbf{\Sigma}_{1|2} = \mathbf{\Sigma}_{11} - \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}\mathbf{\Sigma}_{21}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">FloatText</span><span class="p">,</span> <span class="n">Button</span><span class="p">,</span> <span class="n">HBox</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">Output</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>

<span class="k">class</span> <span class="nc">GaussianProcess</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise_variance</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lengthscale</span> <span class="o">=</span> <span class="n">lengthscale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">signal_variance</span> <span class="o">=</span> <span class="n">signal_variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance</span> <span class="o">=</span> <span class="n">noise_variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K_inv</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cov_pred</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">):</span>
        <span class="n">sqdist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">signal_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">lengthscale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sqdist</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">K_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rbf_kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">K_ss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rbf_kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">))</span>
        <span class="n">mu_s</span> <span class="o">=</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K_inv</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">cov_s</span> <span class="o">=</span> <span class="n">K_ss</span> <span class="o">-</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K_inv</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">mu_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cov_pred</span> <span class="o">=</span> <span class="n">cov_s</span>

    <span class="k">def</span> <span class="nf">add_data_point</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">x</span><span class="p">]])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">y</span><span class="p">]])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_widget</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">output_widget</span><span class="p">:</span>
            <span class="n">output_widget</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cov_pred</span><span class="p">)),</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cov_pred</span><span class="p">)),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data points&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(X)&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">interactive_gp</span><span class="p">(</span><span class="n">lengthscale</span><span class="p">,</span> <span class="n">signal</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">gp</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">lengthscale</span> <span class="o">=</span> <span class="n">lengthscale</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">signal_variance</span> <span class="o">=</span> <span class="n">signal</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">noise_variance</span> <span class="o">=</span> <span class="n">noise</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">gp</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_point</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_input</span><span class="o">.</span><span class="n">value</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y_input</span><span class="o">.</span><span class="n">value</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">add_data_point</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="n">lengthscale_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">&#39;Lengthscale&#39;</span><span class="p">)</span>
<span class="n">signal_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">&#39;Signal&#39;</span><span class="p">)</span>
<span class="n">noise_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">&#39;Noise&#39;</span><span class="p">)</span>

<span class="n">x_input</span> <span class="o">=</span> <span class="n">FloatText</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">&#39;X:&#39;</span><span class="p">)</span>
<span class="n">y_input</span> <span class="o">=</span> <span class="n">FloatText</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">&#39;Y:&#39;</span><span class="p">)</span>
<span class="n">add_button</span> <span class="o">=</span> <span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Add Point&#39;</span><span class="p">)</span>

<span class="n">add_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="n">add_point</span><span class="p">)</span>

<span class="n">controls</span> <span class="o">=</span> <span class="n">VBox</span><span class="p">([</span><span class="n">HBox</span><span class="p">([</span><span class="n">x_input</span><span class="p">,</span> <span class="n">y_input</span><span class="p">,</span> <span class="n">add_button</span><span class="p">])])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Output</span><span class="p">()</span>

<span class="n">display</span><span class="p">(</span><span class="n">controls</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

<span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">()</span>
<span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">gp</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="n">interact</span><span class="p">(</span><span class="n">interactive_gp</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">lengthscale_slider</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signal_slider</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise_slider</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">FloatText</span><span class="p">,</span> <span class="n">Button</span><span class="p">,</span> <span class="n">HBox</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">Output</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="k">class</span> <span class="nc">GaussianProcess</span><span class="p">:</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;ipywidgets&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="key-scenarios-and-insights-in-gaussian-processes">
<h2>Key Scenarios and Insights in Gaussian Processes<a class="headerlink" href="#key-scenarios-and-insights-in-gaussian-processes" title="Link to this heading">#</a></h2>
<p>Consider the following code as a general function space GP code</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">cholesky</span>

<span class="c1"># RBF (Gaussian) kernel function</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Radial Basis Function (RBF) kernel.&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_prime</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x_prime</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">x_prime</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">dist</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># GP prior sampling</span>
<span class="k">def</span> <span class="nf">gp_prior</span><span class="p">(</span><span class="n">kernel_func</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from a GP prior.&quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">+=</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># Add jitter for numerical stability</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">f_prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">f_prior</span><span class="p">,</span> <span class="n">K</span>

<span class="c1"># GP posterior computation</span>
<span class="k">def</span> <span class="nf">gp_posterior</span><span class="p">(</span><span class="n">kernel_func</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the GP posterior mean and covariance.&quot;&quot;&quot;</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Compute covariance matrices</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">K_s</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
    <span class="n">K_ss</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="c1"># Compute the posterior mean and covariance</span>
    <span class="n">K_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">mu_post</span> <span class="o">=</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">K_inv</span> <span class="o">@</span> <span class="n">y_train</span>
    <span class="n">cov_post</span> <span class="o">=</span> <span class="n">K_ss</span> <span class="o">-</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">K_inv</span> <span class="o">@</span> <span class="n">K_s</span>

    <span class="k">return</span> <span class="n">mu_post</span><span class="p">,</span> <span class="n">cov_post</span><span class="p">,</span> <span class="n">K_ss</span>

<span class="c1"># Generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Sample from the GP prior</span>
<span class="n">f_prior</span><span class="p">,</span> <span class="n">K_prior</span> <span class="o">=</span> <span class="n">gp_prior</span><span class="p">(</span><span class="n">rbf_kernel</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Compute the GP posterior</span>
<span class="n">mu_post</span><span class="p">,</span> <span class="n">cov_post</span><span class="p">,</span> <span class="n">K_ss</span> <span class="o">=</span> <span class="n">gp_posterior</span><span class="p">(</span><span class="n">rbf_kernel</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Sample from the GP posterior</span>
<span class="n">L_post</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">cov_post</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f_post</span> <span class="o">=</span> <span class="n">mu_post</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_post</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot the GP prior and covariance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># GP Prior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_prior</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_prior</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Path </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Samples from the GP Prior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">K_prior</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Covariance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Covariance Matrix (Prior)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>

<span class="c1"># GP Posterior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
    <span class="n">mu_post</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span>
    <span class="n">mu_post</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% Confidence Interval&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_post</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_post</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">mu_post</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mean Prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Samples from the GP Posterior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cov_post</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Covariance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Covariance Matrix (Posterior)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/50d9dc6d8ad5851c19cb9fe00a70b5162bb9aed0eb264f002c4c018caa2c89a7.png" src="../../_images/50d9dc6d8ad5851c19cb9fe00a70b5162bb9aed0eb264f002c4c018caa2c89a7.png" />
</div>
</div>
<section id="secenario-1-no-data-gp-prior">
<h3>Secenario 1: No Data (GP Prior)<a class="headerlink" href="#secenario-1-no-data-gp-prior" title="Link to this heading">#</a></h3>
<p><strong>Question:</strong> We want to know that what governs the behavior of a Gaussian Process when no data is available, and how does the prior impact the predictions?</p>
<p><strong>Answer:</strong> When there is no data, the <em>GP prior</em> governs the behavior. The mean function (commonly set to 0) and the kernel (covariance function) dictate the GP’s behavior. The kernel’s hyperparameters, such as length scale and signal variance, play a critical role in shaping the prior.</p>
<ul class="simple">
<li><p>Effect: The GP will represent a random function drawn from the prior. Without observations, it is entirely governed by the kernel, exhibiting smoothness or roughness as per the kernel’s configuration.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the RBF kernel</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">signal_variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">sqdist</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Prior with no data</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">signal_variance</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">K_ss</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">signal_variance</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">K_ss</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;GP Prior (No Data)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d5b34780dd342294ac0111491f489cc9827ca36485d843d05979de91b7ba238c.png" src="../../_images/d5b34780dd342294ac0111491f489cc9827ca36485d843d05979de91b7ba238c.png" />
</div>
</div>
<section id="explanation">
<h4>Explanation<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>When no meaningful training data is provided (or only a single dummy data point), the GP prior dictates the behavior.</p></li>
<li><p>The kernel parameters (e.g., length scale, signal variance) define the shape of the prior mean and covariance.</p></li>
<li><p>The GP has high uncertainty, as reflected by wide confidence intervals.</p></li>
</ul>
</section>
</section>
<section id="scenario-2-similarity-to-one-training-point">
<h3>Scenario 2: Similarity to One Training Point<a class="headerlink" href="#scenario-2-similarity-to-one-training-point" title="Link to this heading">#</a></h3>
<p><strong>Question:</strong> How does a Gaussian Process behave when a test point is highly similar to one of the training points, and what are the implications for the predictive mean and variance?</p>
<p><strong>Answer:</strong> If a test point is similar to a single training point:</p>
<ul class="simple">
<li><p>Covariance: It will show a strong dependency on that training point and negligible correlations with other points. The covariance matrix reflects the kernel’s values between the test point and training data.</p></li>
<li><p>Mean: The predictive mean for the test point will closely align with the observed value at the similar training point.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># One similar training point</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">]])</span>  <span class="c1"># Test point similar to one training point</span>

<span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise_variance</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictive mean at </span><span class="si">{</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">gp</span><span class="o">.</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictive covariance at </span><span class="si">{</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">gp</span><span class="o">.</span><span class="n">cov_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictive mean at 3.0: 0.246
Predictive covariance at 3.0: 0.983
</pre></div>
</div>
</div>
</div>
<section id="id4">
<h4>Explanation<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The test point’s prediction is heavily influenced by the nearest similar training point due to the kernel’s locality.</p></li>
<li><p>The mean aligns closely with the nearby training data, and the covariance reduces near similar points while remaining high elsewhere.</p></li>
</ul>
</section>
</section>
<section id="scenario-3-outlier-data">
<h3>Scenario 3: Outlier Data<a class="headerlink" href="#scenario-3-outlier-data" title="Link to this heading">#</a></h3>
<p><strong>Question:</strong> What happens to the predictive mean and uncertainty of a Gaussian Process when an outlier exists in the training data?</p>
<p><strong>Answer:</strong> Outliers significantly influence the GP:</p>
<ul class="simple">
<li><p>Effect on Mean: The predictive mean may deviate, especially near the outlier.</p></li>
<li><p>Effect on Covariance: Predictive uncertainty increases around the outlier as the model tries to reconcile the discrepancy.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">])</span>  <span class="c1"># Outlier at 0</span>

<span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise_variance</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id5">
<h4>Explanation<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Outliers significantly distort the GP mean, pulling it towards the outlier.</p></li>
<li><p>The covariance increases around the outlier due to uncertainty about whether to trust the point.</p></li>
</ul>
</section>
</section>
<section id="scenario-4-dense-noisy-data">
<h3>Scenario 4: Dense Noisy Data<a class="headerlink" href="#scenario-4-dense-noisy-data" title="Link to this heading">#</a></h3>
<p><strong>Question:</strong> How does observational noise affect the predictions and uncertainties of a Gaussian Process when the training dataset is dense?</p>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>Effect: Observational noise increases uncertainty in predictions, reflected as higher variance in regions with dense noisy data.</p></li>
<li><p>Reason: With dense data, the kernel matrix grows, and inversion for computing predictions becomes sensitive to noise. The noise term (<span class="math notranslate nohighlight">\({\sigma_n}^2 I\)</span>) added to the kernel dominates in regions of high data density.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Adding noise</span>

<span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise_variance</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id6">
<h4>Explanation<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>With a large amount of noisy data, the GP balances fitting the data and modeling uncertainty.</p></li>
<li><p>The effective noise variance increases because the GP assumes part of the variability is due to observation noise.</p></li>
</ul>
</section>
</section>
<section id="general-overview">
<h3>General Overview<a class="headerlink" href="#general-overview" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">cholesky</span>

<span class="c1"># RBF (Gaussian) kernel function</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_prime</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Radial Basis Function (RBF) kernel.&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_prime</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x_prime</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">x_prime</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">dist</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># GP prior sampling</span>
<span class="k">def</span> <span class="nf">gp_prior</span><span class="p">(</span><span class="n">kernel_func</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from a GP prior.&quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">+=</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># Add jitter for numerical stability</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">f_prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">f_prior</span><span class="p">,</span> <span class="n">K</span>

<span class="c1"># GP posterior computation</span>
<span class="k">def</span> <span class="nf">gp_posterior</span><span class="p">(</span><span class="n">kernel_func</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the GP posterior mean and covariance.&quot;&quot;&quot;</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Compute covariance matrices</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">K_s</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
    <span class="n">K_ss</span> <span class="o">=</span> <span class="n">kernel_func</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="c1"># Compute the posterior mean and covariance</span>
    <span class="n">K_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">mu_post</span> <span class="o">=</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">K_inv</span> <span class="o">@</span> <span class="n">y_train</span>
    <span class="n">cov_post</span> <span class="o">=</span> <span class="n">K_ss</span> <span class="o">-</span> <span class="n">K_s</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">K_inv</span> <span class="o">@</span> <span class="n">K_s</span>

    <span class="k">return</span> <span class="n">mu_post</span><span class="p">,</span> <span class="n">cov_post</span><span class="p">,</span> <span class="n">K_ss</span>

<span class="c1"># Generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># Sample from the GP prior</span>
<span class="n">f_prior</span><span class="p">,</span> <span class="n">K_prior</span> <span class="o">=</span> <span class="n">gp_prior</span><span class="p">(</span><span class="n">rbf_kernel</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Compute the GP posterior</span>
<span class="n">mu_post</span><span class="p">,</span> <span class="n">cov_post</span><span class="p">,</span> <span class="n">K_ss</span> <span class="o">=</span> <span class="n">gp_posterior</span><span class="p">(</span><span class="n">rbf_kernel</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Sample from the GP posterior</span>
<span class="n">L_post</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">cov_post</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f_post</span> <span class="o">=</span> <span class="n">mu_post</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_post</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot the GP prior and covariance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># GP Prior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_prior</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_prior</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Path </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Samples from the GP Prior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">K_prior</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Covariance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Covariance Matrix (Prior)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>

<span class="c1"># GP Posterior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
    <span class="n">mu_post</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span>
    <span class="n">mu_post</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% Confidence Interval&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_post</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_post</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">mu_post</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mean Prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Samples from the GP Posterior&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cov_post</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Covariance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Covariance Matrix (Posterior)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">###########################################</span>
<span class="c1"># Updated function to include covariance plots for each scenario</span>
<span class="k">def</span> <span class="nf">plot_gp_scenarios_with_covariance</span><span class="p">(</span><span class="n">X_train_scenarios</span><span class="p">,</span> <span class="n">y_train_scenarios</span><span class="p">,</span> <span class="n">scenario_titles</span><span class="p">,</span> <span class="n">X_test</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot GP predictions and covariance matrices for different training data scenarios.&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X_train_scenarios</span><span class="p">,</span> <span class="n">y_train_scenarios</span><span class="p">,</span> <span class="n">scenario_titles</span><span class="p">)):</span>
        <span class="c1"># Compute GP posterior for each scenario</span>
        <span class="n">mu_post</span><span class="p">,</span> <span class="n">cov_post</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gp_posterior</span><span class="p">(</span><span class="n">rbf_kernel</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">L_post</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">cov_post</span> <span class="o">+</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">f_post</span> <span class="o">=</span> <span class="n">mu_post</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_post</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>

        <span class="c1"># Plot the GP posterior mean and confidence interval</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
            <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
            <span class="n">mu_post</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span>
            <span class="n">mu_post</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_post</span><span class="p">)),</span>
            <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightblue&quot;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% Confidence Interval&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">f_post</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_post</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Sample </span><span class="si">{</span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">mu_post</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mean Prediction&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s2"> - Mean and Prediction&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

        <span class="c1"># Plot the covariance matrix</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cov_post</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Covariance&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s2"> - Covariance Matrix&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test Points&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Define scenarios</span>
<span class="n">X_train_no_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span>  <span class="c1"># No data scenario (dummy single training point)</span>
<span class="n">y_train_no_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

<span class="n">X_train_similar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># Test point similar to one training point</span>
<span class="n">y_train_similar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_train_similar</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">X_train_outlier</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># Outlier scenario</span>
<span class="n">y_train_outlier</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="mi">10</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>

<span class="n">X_train_dense</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Dense noisy data</span>
<span class="n">y_train_dense</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X_train_dense</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train_dense</span><span class="p">))</span>

<span class="c1"># Collect training data and titles</span>
<span class="n">X_train_scenarios</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">X_train_no_data</span><span class="p">,</span>
    <span class="n">X_train_similar</span><span class="p">,</span>
    <span class="n">X_train_outlier</span><span class="p">,</span>
    <span class="n">X_train_dense</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">y_train_scenarios</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">y_train_no_data</span><span class="p">,</span>
    <span class="n">y_train_similar</span><span class="p">,</span>
    <span class="n">y_train_outlier</span><span class="p">,</span>
    <span class="n">y_train_dense</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">scenario_titles</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;No Data (Prior)&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Similarity to One Training Point&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Outlier in Training Data&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Dense Noisy Data&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="c1"># Plot all scenarios with covariance</span>
<span class="n">plot_gp_scenarios_with_covariance</span><span class="p">(</span><span class="n">X_train_scenarios</span><span class="p">,</span> <span class="n">y_train_scenarios</span><span class="p">,</span> <span class="n">scenario_titles</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/50d9dc6d8ad5851c19cb9fe00a70b5162bb9aed0eb264f002c4c018caa2c89a7.png" src="../../_images/50d9dc6d8ad5851c19cb9fe00a70b5162bb9aed0eb264f002c4c018caa2c89a7.png" />
<img alt="../../_images/947402813c2578c491eee87c94871317e61123226e15229aab824250544b904f.png" src="../../_images/947402813c2578c491eee87c94871317e61123226e15229aab824250544b904f.png" />
</div>
</div>
<section id="scenario-overview">
<h4>Scenario Overview<a class="headerlink" href="#scenario-overview" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>No Data (Prior):</p>
<ul class="simple">
<li><p>Mean: Flat with high uncertainty (wide confidence intervals) due to lack of meaningful training data.</p></li>
<li><p>Covariance: Uniformly high correlations between all test points, reflecting the prior assumption.</p></li>
</ul>
</li>
<li><p>Similarity to One Training Point:</p>
<ul class="simple">
<li><p>Mean: Closely follows the single nearby training point, reducing uncertainty near it.</p></li>
<li><p>Covariance: Reduced variance around the training point, but higher correlations elsewhere.</p></li>
</ul>
</li>
<li><p>Outlier in Training Data:</p>
<ul class="simple">
<li><p>Mean: Distorted significantly near the outlier, increasing uncertainty.</p></li>
<li><p>Covariance: Large variance around the outlier region, reflecting less confidence.</p></li>
</ul>
</li>
<li><p>Dense Noisy Data:</p>
<ul class="simple">
<li><p>Mean: Tracks the noisy trend of the training data, balancing fit and smoothness.</p></li>
<li><p>Covariance: Overall reduced due to dense data, but still influenced by noise.</p></li>
</ul>
</li>
</ol>
<section id="key-insights">
<h5>Key Insights<a class="headerlink" href="#key-insights" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>The covariance matrices provide a clear picture of uncertainty and correlation between test points.</p></li>
<li><p>The mean predictions adapt dynamically to the training data, demonstrating GP’s flexibility.</p></li>
</ul>
</section>
</section>
</section>
<section id="numerical-example">
<h3>Numerical Example<a class="headerlink" href="#numerical-example" title="Link to this heading">#</a></h3>
<p>We now demonstrate step-by-step computations of mean and covariance for a Gaussian Process for different scenarios at a test point.</p>
<p>In these scenarios we consider <span class="math notranslate nohighlight">\( X_{test} = [0] \)</span></p>
<section id="scenario-1-no-data-prior">
<h4>Scenario 1: No Data (Prior)<a class="headerlink" href="#scenario-1-no-data-prior" title="Link to this heading">#</a></h4>
<section id="training-data">
<h5>Training Data:<a class="headerlink" href="#training-data" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( X = \begin{bmatrix} 0 \end{bmatrix} \)</span></p>
</section>
<section id="training-targets">
<h5>Training Targets:<a class="headerlink" href="#training-targets" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( y = \begin{bmatrix} 0 \end{bmatrix} \)</span></p>
</section>
<section id="training-covariance-matrix-k">
<h5>Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:<a class="headerlink" href="#training-covariance-matrix-k" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K = \begin{bmatrix} 1.000001 \end{bmatrix}
\]</div>
</section>
<section id="cross-covariance-matrix-k">
<h5>Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:<a class="headerlink" href="#cross-covariance-matrix-k" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K_* = \begin{bmatrix} 1.0 \end{bmatrix}
\]</div>
</section>
<section id="test-covariance-matrix-k">
<h5>Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:<a class="headerlink" href="#test-covariance-matrix-k" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K_{**} = \begin{bmatrix} 1.000001 \end{bmatrix}
\]</div>
</section>
<section id="mean-at-test-point">
<h5>Mean at Test Point:<a class="headerlink" href="#mean-at-test-point" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\mu_* = 0.0
\]</div>
</section>
<section id="variance-at-test-point">
<h5>Variance at Test Point:<a class="headerlink" href="#variance-at-test-point" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\sigma_*^2 = 1.000000000139778
\]</div>
</section>
</section>
<section id="id7">
<h4>Scenario 2: Similarity to One Training Point<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<section id="id8">
<h5>Training Data:<a class="headerlink" href="#id8" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( X = \begin{bmatrix} -3 \\ 0 \\ 3 \end{bmatrix} \)</span></p>
</section>
<section id="id9">
<h5>Training Targets:<a class="headerlink" href="#id9" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( y = \begin{bmatrix} -0.1411 \\ 0.0 \\ 0.1411 \end{bmatrix} \)</span></p>
</section>
<section id="id10">
<h5>Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:<a class="headerlink" href="#id10" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[\begin{split}
K = \begin{bmatrix}
1.000001 &amp; 0.011109 &amp; 0.00012341 \\
0.011109 &amp; 1.000001 &amp; 0.011109 \\
0.00012341 &amp; 0.011109 &amp; 1.000001
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="id11">
<h5>Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:<a class="headerlink" href="#id11" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[\begin{split}
K_* = \begin{bmatrix} 0.011109 \\ 1.0 \\ 0.011109 \end{bmatrix}
\end{split}\]</div>
</section>
<section id="id12">
<h5>Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:<a class="headerlink" href="#id12" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K_{**} = \begin{bmatrix} 1.000001 \end{bmatrix}
\]</div>
</section>
<section id="id13">
<h5>Mean at Test Point:<a class="headerlink" href="#id13" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\mu_* = 0.0
\]</div>
</section>
<section id="id14">
<h5>Variance at Test Point:<a class="headerlink" href="#id14" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\sigma_*^2 = 9.999889812758412 \times 10^{-7}
\]</div>
</section>
</section>
<section id="scenario-3-outlier-in-training-data">
<h4>Scenario 3: Outlier in Training Data<a class="headerlink" href="#scenario-3-outlier-in-training-data" title="Link to this heading">#</a></h4>
<section id="id15">
<h5>Training Data:<a class="headerlink" href="#id15" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( X = \begin{bmatrix} -3 \\ -2 \\ 0 \\ 1 \\ 2 \\ 3 \end{bmatrix} \)</span></p>
</section>
<section id="id16">
<h5>Training Targets:<a class="headerlink" href="#id16" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( y = \begin{bmatrix} -0.1411 \\ -0.9093 \\ 10.0 \\ 0.8415 \\ 0.9093 \\ 0.1411 \end{bmatrix} \)</span></p>
</section>
<section id="id17">
<h5>Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:<a class="headerlink" href="#id17" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[\begin{split}
K = \begin{bmatrix}
1.000001 &amp; 0.606531 &amp; 0.011109 &amp; 0.00033546 &amp; 0.0000037267 &amp; 0.00000012341 \\
0.606531 &amp; 1.000001 &amp; 0.135335 &amp; 0.011109 &amp; 0.00033546 &amp; 0.0000037267 \\
0.011109 &amp; 0.135335 &amp; 1.000001 &amp; 0.135335 &amp; 0.011109 &amp; 0.00033546 \\
0.00033546 &amp; 0.011109 &amp; 0.135335 &amp; 1.000001 &amp; 0.135335 &amp; 0.011109 \\
0.0000037267 &amp; 0.00033546 &amp; 0.011109 &amp; 0.135335 &amp; 1.000001 &amp; 0.606531 \\
0.00000012341 &amp; 0.0000037267 &amp; 0.00033546 &amp; 0.011109 &amp; 0.606531 &amp; 1.000001
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="id18">
<h5>Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:<a class="headerlink" href="#id18" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[\begin{split}
K_* = \begin{bmatrix} 0.011109 \\ 0.135335 \\ 1.0 \\ 0.135335 \\ 0.011109 \\ 0.00033546 \end{bmatrix}
\end{split}\]</div>
</section>
<section id="id19">
<h5>Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:<a class="headerlink" href="#id19" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K_{**} = \begin{bmatrix} 1.000001 \end{bmatrix}
\]</div>
</section>
<section id="id20">
<h5>Mean at Test Point:<a class="headerlink" href="#id20" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\mu_* = 0.1797
\]</div>
</section>
<section id="id21">
<h5>Variance at Test Point:<a class="headerlink" href="#id21" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\sigma_*^2 = 9.967806621528498 \times 10^{-7}
\]</div>
</section>
</section>
<section id="id22">
<h4>Scenario 4: Dense Noisy Data<a class="headerlink" href="#id22" title="Link to this heading">#</a></h4>
<section id="id23">
<h5>Training Data:<a class="headerlink" href="#id23" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( X = \begin{bmatrix} -5.0 \\ -4.4737 \\ \vdots \\ 5.0 \end{bmatrix} \)</span></p>
</section>
<section id="id24">
<h5>Training Targets:<a class="headerlink" href="#id24" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\( y = \begin{bmatrix} -1.4015 \\ -1.0347 \\ \vdots \\ 0.1788 \end{bmatrix} \)</span></p>
</section>
<section id="id25">
<h5>Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:<a class="headerlink" href="#id25" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K = \text{(20x20 matrix)}
\]</div>
</section>
<section id="id26">
<h5>Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:<a class="headerlink" href="#id26" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K_* = \text{(20x1 matrix)}
\]</div>
</section>
<section id="id27">
<h5>Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:<a class="headerlink" href="#id27" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
K_{**} = \begin{bmatrix} 1.000001 \end{bmatrix}
\]</div>
</section>
<section id="id28">
<h5>Mean at Test Point:<a class="headerlink" href="#id28" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\mu_* = -0.0514
\]</div>
</section>
<section id="id29">
<h5>Variance at Test Point:<a class="headerlink" href="#id29" title="Link to this heading">#</a></h5>
<div class="math notranslate nohighlight">
\[
\sigma_*^2 = 0.0126
\]</div>
</section>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./StudentEffort\GP_Midterm"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../ridge/ridge.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Ridge Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="../../Contact_Me.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Contact Me</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Midterm Exam - Gaussian Processes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#course-machine-learning">Course: Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semester-fall-1403">Semester: Fall 1403</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#authors"><strong>Authors</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-setup">1. <strong>Model Setup</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution-on-weights">2. <strong>Prior Distribution on Weights</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-function">3. <strong>Likelihood Function</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-distribution-of-weights-training-phase">4. <strong>Posterior Distribution of Weights</strong> (<strong>Training Phase</strong>)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-derivation-of-the-posterior-distribution">Step-by-Step Derivation of the Posterior Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-0-what-is-the-product-of-two-gaussian-distributions"><strong>Step 0: What is the product of two Gaussian distributions?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-combine-the-exponentials"><strong>Step 1: Combine the Exponentials</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-expand-the-quadratic-terms"><strong>Step 2: Expand the Quadratic Terms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-term-y-w-top-x-top-y-w-top-x">First Term: <span class="math notranslate nohighlight">\(( (y - w^\top X)^\top (y - w^\top X) )\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#second-term-frac-1-2-w-top-sigma-p-1-w">Second Term: <span class="math notranslate nohighlight">\(( -\frac{1}{2} w^\top \Sigma_p^{-1} w )\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-combine-all-terms"><strong>Step 3: Combine All Terms</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-complete-the-square"><strong>Step 4: Complete the Square</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-derivative-gradient">First Derivative (Gradient)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#second-derivative-hessian">Second Derivative (Hessian)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-final-posterior-distribution"><strong>Step 5: Final Posterior Distribution</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-key-formulas"><strong>Summary of Key Formulas:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution-of-f">5. <strong>Predictive Distribution of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-mean-of-f"><strong>Predictive mean of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-variance-of-f"><strong>Predictive Variance of <span class="math notranslate nohighlight">\(( f_* )\)</span></strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-predictive-distribution">6. <strong>Final Predictive Distribution</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Summary of Key Formulas:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#handle-high-dimensional-and-kernels">Handle High-Dimensional and Kernels</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-kernels-in-gaussian-processes">When to Use Kernels in Gaussian Processes?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-define-the-covariance-between-data-points">1. <strong>To Define the Covariance Between Data Points</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-encode-assumptions-about-the-function-being-modeled">2. <strong>To Encode Assumptions About the Function Being Modeled</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-model-nonlinear-relationships">3. <strong>To Model Nonlinear Relationships</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-incorporate-prior-knowledge-about-the-data">4. <strong>To Incorporate Prior Knowledge about the Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-express-uncertainty-in-predictions">5. <strong>To Express Uncertainty in Predictions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-handle-high-dimensional-or-complex-input-spaces">6. <strong>To Handle High-Dimensional or Complex Input Spaces</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-model-complex-data-structures">7. <strong>To Model Complex Data Structures</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-achieve-non-parametric-flexibility">8. <strong>To Achieve Non-Parametric Flexibility</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-why-kernels-are-essential-in-gaussian-processes">Summary of Why Kernels Are Essential in Gaussian Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-kernel-functions-used-in-gaussian-processes">Common Kernel Functions Used in Gaussian Processes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-kernel-functions-in-gaussian-processes">Example of Kernel Functions in Gaussian Processes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-kernel-trick">Apply Kernel Trick</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-recall-the-prior-and-likelihood">Step 1: Recall the prior and likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-posterior-distribution">Step 2: Posterior Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-expressing-in-kernel-form">Step 3: Expressing in Kernel Form</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-mean-in-kernel-form">3.1 Posterior Mean in Kernel Form</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-variance-in-kernel-form">3.2 Posterior Variance in Kernel Form</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-result">Final Result</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-space-view-non-parametric-in-gaussian-processes">Function-Space View (Non parametric) in Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-over-functions">1. Prior Over Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-normal-distribution">2. Joint Normal Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-pdf">Joint PDF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-distribution">Marginal Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-distribution">Conditional Distribution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-over-functions">3. Posterior Over Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#incorporate-observational-noise">Incorporate Observational Noise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-example-of-gaussian-process-regression">Numerical Example of Gaussian Process Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-setup">Problem Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-compute-covariance-matrices">Step 1: Compute Covariance Matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-covariance-matrix-mathbf-k">1. Training covariance matrix <span class="math notranslate nohighlight">\(( \mathbf{K} )\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#test-covariance-vector-mathbf-k">2. Test covariance vector <span class="math notranslate nohighlight">\(( \mathbf{k}_* )\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example-for-x-4">Example for ( x_* = -4 ):</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-for-all-test-points">Computation for all test points:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-adjusted-covariance-matrix-mathbf-k-y">3. Noise-adjusted covariance matrix <span class="math notranslate nohighlight">\(( \mathbf{K}_y )\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-predictive-mean-and-variance">Step 2: Predictive Mean and Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-mean-mu-x">1. Predictive mean <span class="math notranslate nohighlight">\(( \mu(x_*) )\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-variance-sigma-2-x">2. Predictive variance <span class="math notranslate nohighlight">\(( \sigma^2(x_*) )\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-predictions-for-all-test-points">Step 3: Predictions for All Test Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-confidence-intervals">Step 4: Confidence Intervals</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Example:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-conditional-distribution-formula">Proof of Conditional Distribution Formula</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-scenarios-and-insights-in-gaussian-processes">Key Scenarios and Insights in Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#secenario-1-no-data-gp-prior">Secenario 1: No Data (GP Prior)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-2-similarity-to-one-training-point">Scenario 2: Similarity to One Training Point</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-3-outlier-data">Scenario 3: Outlier Data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-4-dense-noisy-data">Scenario 4: Dense Noisy Data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-overview">General Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-overview">Scenario Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insights">Key Insights</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-example">Numerical Example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-1-no-data-prior">Scenario 1: No Data (Prior)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#training-targets">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#training-covariance-matrix-k">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-covariance-matrix-k">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#test-covariance-matrix-k">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-at-test-point">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-at-test-point">Variance at Test Point:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Scenario 2: Similarity to One Training Point</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Variance at Test Point:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-3-outlier-in-training-data">Scenario 3: Outlier in Training Data</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Variance at Test Point:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Scenario 4: Dense Noisy Data</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Training Data:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Training Targets:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Training Covariance Matrix <span class="math notranslate nohighlight">\( K \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Cross Covariance Matrix <span class="math notranslate nohighlight">\( K_* \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Test Covariance Matrix <span class="math notranslate nohighlight">\( K_{**} \)</span>:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Mean at Test Point:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Variance at Test Point:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr.Hadi Sadoghi Yazdi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 Pattern Recognition Lab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>